---------------------CHAINLIT-----------------------
Chainlit is an open-source Python framework that helps you easily build chatbot-style AI applications without needing to know frontend development.

ğŸ”¹ Main Idea:
It lets you create a user interface (UI) for conversational AI apps â€” like chatbots or virtual assistants â€” directly in Python. You focus on your AI logic, and Chainlit automatically handles how it looks and interacts with users.

ğŸ”¹ Key Features:
Python-First: Everything (logic + UI) is written in Python â€” no HTML or JavaScript needed.
Real-Time Interaction: Supports instant message exchange between users and the AI.
Rich UI Elements: You can include text, images, audio, video, buttons, forms, and even charts.
State Management: Remembers conversation history or user preferences.
Async Support: Handles multiple users or long model responses smoothly.
Middleware & Hooks: Lets you add custom logic (like authentication or logging).
Easy Deployment: Run locally or on cloud with minimal setup.
Open Source: Community-driven with regular updates.

ğŸ”¹ Analogy:
Think of Chainlit as Streamlit for chatbots â€” both are simple Python tools for building interactive apps, but Chainlit is specialized for conversational AI.

ğŸ”¹ Who Uses It:
AI developers & ML engineers building chatbots or assistants
Data scientists testing conversational models
Teachers & researchers exploring AI communication

ğŸ‘‰ In short:
Chainlit makes it super easy to build, test, and deploy AI chat interfaces directly in Python â€” fast, interactive, and perfect for conversational AI projects.





------------------------------------------CHAINLIT SIMPLE CODE EXPLAIN-----------------------------------
https://github.com/panaversity/learn-agentic-ai/tree/main/01_ai_agents_first/25_chainlit/helloworld
(LINK FOR CHAINLIT INSTALLATION)


  import chainlit as cl

@cl.on_message
async def main(message: cl.Message):
    # Our custom logic goes here...
    # Send a fake response back to the user
    await cl.Message(
        content=f"Received: {message.content}",
    ).send()

  ----------------------------------------------------------------------------

Line 1 â€” import chainlit as cl
import â€” Python keyword that loads a module so you can use its functions, classes, etc.

chainlit â€” the package name. Itâ€™s a framework that exposes classes and decorators for building chat-like apps.
as â€” renames the module for this file.
cl â€” the local alias. After this, every reference to cl means chainlit.
Effect: You can now call cl.something instead of chainlit.something.

Line 3 â€” @cl.on_message
@ â€” Python decorator syntax. It wraps the function below and registers or modifies it.
cl.on_message â€” a decorator provided by Chainlit. It registers the function as a handler that the Chainlit runtime will call whenever a message from a user arrives.(yh decorator message
anay per call hoga)
Effect: The main function becomes the callback for incoming messages.

Line 4 â€” async def main(message: cl.Message):(asyn mtlb backend per kam hota rhega jb kam hojyga to yh response return krdega
Break into pieces:
async â€” marks the function as a coroutine (asynchronous function). It can await other coroutines without blocking the whole server.
  async â€” easy meaning
When you put async before a function, it means:
â¡ï¸ â€œThis function can pause and wait for something (like a message send or API call) without freezing the whole app.â€
Example idea
Imagine youâ€™re cooking and waiting for water to boil.
A normal function would just stand there doing nothing until the water boils (blocking).
An async function can say, â€œIâ€™ll come back when itâ€™s ready,â€ and meanwhile do other tasks (non-blocking).
So in this case,
async def main(...) means your function can wait for messages to send or get data from somewhere while the server keeps running smoothly for other users.
def â€” defines a function.
main â€” function name. This is the handler that Chainlit will call.
(message: cl.Message) â€” function parameter with a type annotation:
message â€” the variable that will receive the incoming message object.
: and cl.Message â€” annotation saying message is expected to be an instance of cl.Message (a Chainlit class representing a message).(message ki class h cl.message mtlb jo input h wo
text h message h)
: â€” ends the function signature and begins the function body.
Effect: Chainlit calls main(...) with a cl.Message object when a user message arrives. Because itâ€™s async, the function can perform asynchronous I/O (send replies, query DB, etc.) without blocking.

Line 5 â€” # Our custom logic goes here...
# â€” starts a comment. Python ignores everything after # on that line.
This comment indicates where you would implement your real processing logic (NLP, DB lookups, business rules, etc.).

Line 6 â€” # Send a fake response back to the user
Another comment explaining what the next statement does.

Line 7 â€” await cl.Message(
Breakdown:(cl.message jo h wo message ka object return krta h mtlb output bhi text message hoga )
await â€” pauses this coroutine until the awaited coroutine completes. Only valid inside async functions.
cl.Message(...) â€” constructs a new Message object from Chainlit. This represents an outgoing message you intend to send back to the user.
cl â€” the module alias.
Message â€” a class or factory in Chainlit to create messages.
Effect: We create a message object and then await ... .send() (explained next) to actually deliver it.
Inside the Message constructor â€” content=f"Received: {message.content}",
content= â€” a keyword argument setting the message body (what the user will see).
f"Received: {message.content}" â€” an f-string (formatted string literal):
f"..." â€” allows inlining expressions inside {}.
Received: â€” literal text.
{message.content} â€” inserts the content attribute of the incoming message object (the userâ€™s text).
The trailing comma after the argument is allowed; itâ€™s just the single kwarg.
Effect: The outgoing message text becomes Received: <what user typed>.
End of line â€” ).send()(.send() lgany se yh smny waly ka msg bn jata h jo smny se ayga)
) â€” closes the cl.Message(...) constructor call.
.send() â€” calls the send method on the Message object. In Chainlit, .send() is an async method (a coroutine) that transmits the message back through the Chainlit server to the connected client(s).
Because .send() is a coroutine, we await it (see earlier await) â€” that actually runs the send operation.(return ki jagah await lgta h async or await lgany se async function bn jata h)
Effect: The constructed message is delivered to the user.
How it all connects at runtime (flow)
Chainlit framework runs an event loop and a server (websocket/HTTP) that accepts user connections.
When a user sends a message to the app, Chainlit creates a cl.Message instance representing that incoming message and invokes any registered handlers â€” here, your main function (registered by @cl.on_message).
Chainlit calls await main(message) (it schedules the coroutine).
Inside main, you build a reply with cl.Message(content=...) and await ...send() to send it back.
The user sees the reply in their chat client.

Notes / quick tips
async + await let you perform I/O (database calls, HTTP requests, file ops, .send()) without blocking other users.
@cl.on_message hooks your function into Chainlitâ€™s message event system â€” you can register multiple handlers or use other decorators for different events (depending on Chainlit features).
message usually has other useful attributes besides .content (author, id, metadata). Consult Chainlit docs for details.
To run this, you generally start a Chainlit app (e.g., chainlit run <your_file>.py) â€” Chainlit provides the server that triggers the decorator.



----------------------------------CODE OF CHAINLIT + OPENAI SDK--------------------------------
import chainlit as cl
from agents import Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel
from agents import function_tool
import os
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)





@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the Panadversity Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = await Runner.run(
        agent, 
        input=history,
        run_config = config,
    )

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)
   
    await cl.Message(
        content=result.final_output
    ).send()
