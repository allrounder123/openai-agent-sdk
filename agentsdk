
-----------agent jo h wo autonomous action lene wala ek model h--------------------------
------async mtlb ham request bhjty hn wo process krta h or jb uska process khtm hoga wo response dedega-------------------------
--------openai ky sdk ko use krty huy ham apny customize llm ko use krty hn jese openai m gemini flash use krty-----------------sdk agent or sary ky sary features sdk ky hongy lekin 
backend per jo llm hoga wo free wala hoga gemini ka-------------demagh jo h wo gemini ka or jo decision making wagaira h wo openai sdk ka h
-----------sync mtlb task jb perform horha h ussi wqt response do , or async mtlb 10 15 mnt bhi sochlo or jb response complete hojye tb response dedo
--------Runner m bht sary function method hoty by default async hota or ham Runner.run_sync krty hn (run_sync method h Runner m)(client to server communication jo krwaty hn wo sary 
tareeky runner m mojud hn)
------sync mtlb mene apko kaha biryani lekr ao to jb tk ap biryani ni laogy tb tk m wait krungi kisi  se bt ni krungi intezar krungi or async mtlb mene kaha ap biryani lekr ao or jb tk ap
ni aty tb tk m baqi kaam nipta lungi bt krlungi sbse kaam krlungi




-----------------------
In the OpenAI Agents SDK (and similar frameworks), youâ€™ll often see three levels of logic:
Global level â†’ Agent level â†’ Run level

Letâ€™s break each down simply ğŸ‘‡
ğŸŒ Global Level
The topmost layer â€” things that apply to the whole app or system, not just one agent or one conversation.
It defines shared settings, global tools, and event handlers that affect everything.

Think of it as:
ğŸ§  â€œWhat rules or tools should all agents or runs in my app have access to?â€

Examples:
Registering a tool that every agent can use (like a database connector).
Defining logging or error-handling behavior for the whole app.
Setting up startup events, environment variables, or authentication.

ğŸ§© Agent Level
Defines the behavior and configuration of one specific agent.
Example: system prompt, tool access, goals, or memory setup.
Agents can inherit or override global settings.

Think of it as:
ğŸ¤– â€œWho the agent is and what it can do.â€

âš™ï¸ Run Level
The execution layer â€” what happens in a single conversation or task run.
This is where the agent actually processes input, calls tools, and produces outputs.

Think of it as:
ğŸƒ â€œWhat happens right now while the agent is running.â€





---------------------SYNC FUNCTION IN RUNNER--------------(WORK ON GOOGLE COLAB)--------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

math_agent: Agent = Agent(name="MathAgent",
                     instructions="You are a helpful math assistant.",
                     model=llm_model) # gemini-2.5 as agent brain - chat completions

result: Runner = Runner.run_sync(math_agent, "why learn math for AI Agents?", run_config=config)

print("\nCALLING AGENT\n")
print(result.final_output)




-------------------------------------------------------------------------------------------------------------------------------------
-------------ASYNC FUNCTION IN RUNNER----------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio(is library se koi bhi async function chla skty hn)

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)---(run time configuration)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = await Runner.run(agent, "Tell me about recursion in programming.", run_config=config)-----------(.run ka method Runner m by-default async hi hota h)( or await mtlb jo kam 
krrha h uska wait krega or jb wo response ajyga usko result m dal dega)
    print(result.final_output)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.

if __name__ = "__main__":(is line ki zrort .py file m hoti h jo vs or cursor m hota h yhn iski zrort ni h kunke google colab m import asyncio krdia h hamne jo async ky sary function chlata ha
asyncio.run(main())



--------------chatgpt foran reply krta h to wo async h agr kisi kam per soch kr phr response kry to wo sync

-------------------STREAMED FUNCTION IN RUNNER-----------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = Runner.run_streamed(agent, "Tell me about recursion in programming.", run_config=config)
    print(result)
    async for e in result.stream_events():
      print(e)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.


asyncio.run(main())


-----------------------------------streamed code run in google colab---------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
import os
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
from agents.run import RunConfig
from google.colab import userdata
import asyncio

from agents import (
    Agent,
    Runner,
    set_default_openai_api,
    set_default_openai_client,
    set_tracing_disabled,
)

gemini_api_key = userdata.get("GEMINI_API_KEY")


# Check if the API key is present; if not, raise an error
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.")

#Reference: https://ai.google.dev/gemini-api/docs/openai
external_client = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

model = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

set_default_openai_client(client=external_client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner


async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
        model=model
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):-----------------(ResponseTextDeltaEvent: event data ky class ki type bhi hogi yh type h 
data ki jo wo dega,raw_response_event:loop lgaya h jo us betahasha chizain arhi hn to hamne kaha raw response event jb hoga us wqt wo kam complete krke data return krta h)
            print(event.data.delta, end="", flush=True)-----------(event.data.delta: yh text nikalyga jitna complete hua h
 ( end=""----: by default print jb horha hota h to wo \n se next line per ajata h to hamne kehdia ky next line ni chaiye jo arha h usy dusry ky agay likhty jao space dal kr

asyncio.run(main())



------In Python, flush=True in the print() function forces the output to be written to the console immediately. Normally Python buffers output, which means it might delay displaying 
what you printed until the buffer is full or a newline character is encountered. Using flush=True is particularly useful in scenarios like real-time logging or when printing streaming
data, ensuring that every piece of output is displayed as soon as it's generated.

For example, in your code:
print(event.data.delta.end="", flush=True)

end="": This means no newline character is appended at the end of the printed text.
flush=True: This forces the output to be flushed to the console immediately, even without a newline

------------------------FLUSH=TURE IN OUR CODE JO UPR GYA H------------------------
ğŸ§  Meaning of flush=True
Normally, Python buffers printed text â€” meaning it stores it temporarily before actually showing it on the screen.
So when you print inside a loop, the text might not appear immediately.
âœ… flush=True forces Python to immediately push (flush) the text from the buffer to the console or output stream.

ğŸ’¬ In this context (streaming output)
Here, the model sends chunks of text as it generates them (event.data.delta = partial text).
To make the output appear live, like real-time typing, we use:
flush=True

That way, each small piece (token or word) appears instantly instead of waiting until the model finishes.

âš™ï¸ Example to visualize it
Without flush:
# Text appears all at once at the end
print("H", end="")
print("e", end="")
print("l", end="")
print("l", end="")
print("o", end="")

â†’ Output might show â€œHelloâ€ only after the loop ends.
With flush:
# Text appears letter by letter
print("H", end="", flush=True)
print("e", end="", flush=True)
print("l", end="", flush=True)
print("l", end="", flush=True)
print("o", end="", flush=True)

â†’ Output shows H e l l o in real-time.

ğŸ§¾ Summary
flush=True â†’ forces the output to appear immediately.
Used in streaming to show model text live as itâ€™s generated.
Without it, the console might delay or group text output.

âœ… In short:
flush=True makes the text appear instantly during streaming â€” creating that smooth, â€œtyping liveâ€ effect.




----------------------stream ky ilawa event bhi perform horhy hoty h kunke agent ko hamne task dia to wo bht sary kaam krrha hota ha tool ko cl bhi krta h uska output bhi lata h to bht
sary event perform horhy hoty hn hamne upr waly code m to bs ek event nikala tha jis m wo text complete krke return krta ha to kon kon se event perform hoty hn agr unhy dekhna chahain to?
same steps wohi hn bs thori changes hn jo yh nichy waly code m hui h-----------------------------------------------------------

import asyncio
import random

from agents import Agent, ItemHelpers, Runner, function_tool


@function_tool--------(yh tool decorator function use kia h function ko tool bnadeta h yh)
def how_many_jokes() -> int:------------(yh normal function h jo random number generate krta ha)
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],-------------(jo tool bnaya tha upr wo hamne yhn apne agent ko provide krdia)
        model=model
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",

    )
    print("=== Run starting ===")
    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types





asyncio.run(main())

print("=== Run complete ===")


---------------------chainlit+stream runner in currsor--------------------------------------
import chainlit as cl
from agents import Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel
from agents import function_tool
import os
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)





@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the Panadversity Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    msg = cl.Message(content="")
    await msg.send()
    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = Runner.run_streamed(
        agent, 
        input=history,
        run_config = config,
    )
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            token = event.data.delta
            await msg.stream_token(token)

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)
   
 

--------------------------------------------------------------------------------------------
-------------------IMPORTANT:
Thatâ€™s one of the 3 main ways to run an agent in the Agents SDK:

run_sync â†’ normal, blocking
run_async â†’ async, non-blocking
run_streamed â†’ streaming, real-time output


âš¡ What is â€œstreamingâ€?
Streaming means:
The model starts sending partial output while itâ€™s still generating the rest.
So instead of waiting for the whole response (like sync/async do), your code receives chunks (events) of the output as soon as theyâ€™re available.

Think of it like:
Non-streamed (sync/async): Wait till the whole essay is done â†’ print once.
Streamed: Show words as theyâ€™re typed â†’ print gradually, real-time.

ğŸ§  Runner.run_streamed() â€” what it does
It starts an asynchronous run.
Immediately returns a streaming result object (not the final text).
That result object emits events as the model generates output.

---------------You can then do:CODE-----------------

async for e in result.stream_events():
    print(e)

â†’ This listens for events like tokens, messages, or tool calls as they happen.
This is perfect for live chat UIs (like Chainlit, Gradio, or Colab outputs).

------------------------------------------------------------------------------

ğŸ”„ Runner.run_async() â€” what it does
Runs the same logic but waits until the full response is ready before returning.
You can await it to get the entire message result directly.
No live token-by-token updates.

Example:

result = await Runner.run_async(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you just need the full answer, not a live stream.
--------------------------------------------------------------------------------------

â³ Runner.run_sync() â€” what it does
Same as run_async but for non-async (normal) Python code.
Blocks until the full run completes (like a normal function).

Example:

result = Runner.run_sync(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: youâ€™re in a plain Python script (no asyncio).




---------------------------------------------------------------------
ğŸ’¬ In your code:

result = Runner.run_streamed(agent, "Tell me about recursion...", run_config=config)

Starts the agent.
Doesnâ€™t wait for full completion.
Gives you a stream handle in result.

Then this part:

async for e in result.stream_events():
    print(e)

prints the tokens or partial responses live, as they arrive.
Thatâ€™s how you get â€œlive typingâ€ or real-time generation.


---------------------------------------------
ğŸ§  What is Runner?
Runner is a controller class in the OpenAI Agents SDK thatâ€™s responsible for executing (running) an agent.
Think of it as the engine that takes:
your Agent (the brain),
a user input (the question),
a RunConfig (the setup),
and actually runs the full conversation or reasoning process.

âš™ï¸ In short:
Runner = the component that manages how an agentâ€™s logic is executed â€” including tool calls, message handling, and LLM requests.
---runner jo h wo client to server communication krwata ha or communication sync hogi asyn hogi ya streamed hogi yh define krty.


---------------------------------------
ğŸ§± Analogy â€” AI system as a car
Agent â†’ The driver â€” knows what to do and how to respond.
RunConfig â†’ The GPS/settings â€” defines which model, API key, and configuration to use.
Runner â†’ The engine â€” actually runs the process and makes the agent work.
Without the Runner, the agent just exists but canâ€™t perform any actions.

ğŸ§° Runnerâ€™s Main Methods
run_sync() â†’ Runs the agent normally (blocking execution until done).
run_async() â†’ Runs the agent asynchronously using await (non-blocking).
run_streamed() â†’ Runs the agent in streaming mode, giving partial outputs live while generating.







---------------------TOOL CALLING------------------------------
In AI, a tool is an external helper function or service that an AI model can use to perform specific tasks it canâ€™t do on its own â€” like checking the weather, running code, searching
the web, or doing calculations.
The process of tool calling means the AI automatically decides when and how to use these tools during a conversation.
Itâ€™s like the AI saying:
â€œI canâ€™t do this myself, but I know a tool that can!â€
Then it calls the tool, gets the result, and uses it to respond naturally.

ğŸ§­ Why Tools Are Important
ğŸ•’ Fetch live data â€” e.g., current weather, stock prices, or news.
âš™ï¸ Perform actions â€” e.g., send an email, run Python code, access a database.
âœ… Improve accuracy and safety â€” by verifying answers using reliable sources.

ğŸ“ Summary (in points)
A tool is a helper function that performs one specific task.
Tool calling is when the AI uses that helper automatically during chat.
Tools extend what AI can do (fetch real data, act, or validate info).
The AI uses tools just like a person uses a calculator â€” decides when to use it, gets a result, and explains it.
Tools make AI more useful, interactive, and reliable.


----------------------------------------simple tools code------------------------------------------
from agents.tool import function_tool

@function_tool("get_weather")
def get_weather(location: str, unit: str = "C") -> str:
    """
    fetch the weather for a given location, returning a short description.

    # Example Logic
    return f"The weather in {location} is 22 degrees {unit}."
    """


@function_tool("puaic_student_finder")
def student_finder(student_roll: int) -> str:
    """
    find the PUAIC student based on the roll number

    Data = {1: "Basim", 2: "Sir Zia", 3: "Amaan"}
return data.get(student_roll,"not found")
