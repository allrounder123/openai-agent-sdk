
-----------agent jo h wo autonomous action lene wala ek model h--------------------------
------async mtlb ham request bhjty hn wo process krta h or jb uska process khtm hoga wo response dedega-------------------------
--------openai ky sdk ko use krty huy ham apny customize llm ko use krty hn jese openai m gemini flash use krty-----------------sdk agent or sary ky sary features sdk ky hongy lekin 
backend per jo llm hoga wo free wala hoga gemini ka-------------demagh jo h wo gemini ka or jo decision making wagaira h wo openai sdk ka h
-----------sync mtlb task jb perform horha h ussi wqt response do , or async mtlb 10 15 mnt bhi sochlo or jb response complete hojye tb response dedo
--------Runner m bht sary function method hoty by default async hota or ham Runner.run_sync krty hn (run_sync method h Runner m)(client to server communication jo krwaty hn wo sary 
tareeky runner m mojud hn)
------sync mtlb mene apko kaha biryani lekr ao to jb tk ap biryani ni laogy tb tk m wait krungi kisi  se bt ni krungi intezar krungi or async mtlb mene kaha ap biryani lekr ao or jb tk ap
ni aty tb tk m baqi kaam nipta lungi bt krlungi sbse kaam krlungi




-----------------------
In the OpenAI Agents SDK (and similar frameworks), youâ€™ll often see three levels of logic:
Global level â†’ Agent level â†’ Run level

Letâ€™s break each down simply ğŸ‘‡
ğŸŒ Global Level
The topmost layer â€” things that apply to the whole app or system, not just one agent or one conversation.
It defines shared settings, global tools, and event handlers that affect everything.

Think of it as:
ğŸ§  â€œWhat rules or tools should all agents or runs in my app have access to?â€

Examples:
Registering a tool that every agent can use (like a database connector).
Defining logging or error-handling behavior for the whole app.
Setting up startup events, environment variables, or authentication.

ğŸ§© Agent Level
Defines the behavior and configuration of one specific agent.
Example: system prompt, tool access, goals, or memory setup.
Agents can inherit or override global settings.

Think of it as:
ğŸ¤– â€œWho the agent is and what it can do.â€

âš™ï¸ Run Level
The execution layer â€” what happens in a single conversation or task run.
This is where the agent actually processes input, calls tools, and produces outputs.

Think of it as:
ğŸƒ â€œWhat happens right now while the agent is running.â€





---------------------SYNC FUNCTION IN RUNNER--------------(WORK ON GOOGLE COLAB)--------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

math_agent: Agent = Agent(name="MathAgent",
                     instructions="You are a helpful math assistant.",
                     model=llm_model) # gemini-2.5 as agent brain - chat completions

result: Runner = Runner.run_sync(math_agent, "why learn math for AI Agents?", run_config=config)

print("\nCALLING AGENT\n")
print(result.final_output)




-------------------------------------------------------------------------------------------------------------------------------------
-------------ASYNC FUNCTION IN RUNNER----------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio(is library se koi bhi async function chla skty hn)

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)---(run time configuration)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = await Runner.run(agent, "Tell me about recursion in programming.", run_config=config)-----------(.run ka method Runner m by-default async hi hota h)( or await mtlb jo kam 
krrha h uska wait krega or jb wo response ajyga usko result m dal dega)
    print(result.final_output)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.

if __name__ = "__main__":(is line ki zrort .py file m hoti h jo vs or cursor m hota h yhn iski zrort ni h kunke google colab m import asyncio krdia h hamne jo async ky sary function chlata ha
asyncio.run(main())



--------------chatgpt foran reply krta h to wo async h agr kisi kam per soch kr phr response kry to wo sync

-------------------STREAMED FUNCTION IN RUNNER-----------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = Runner.run_streamed(agent, "Tell me about recursion in programming.", run_config=config)
    print(result)
    async for e in result.stream_events():
      print(e)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.


asyncio.run(main())


-----------------------------------streamed code run in google colab---------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
import os
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
from agents.run import RunConfig
from google.colab import userdata
import asyncio

from agents import (
    Agent,
    Runner,
    set_default_openai_api,
    set_default_openai_client,
    set_tracing_disabled,
)

gemini_api_key = userdata.get("GEMINI_API_KEY")


# Check if the API key is present; if not, raise an error
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.")

#Reference: https://ai.google.dev/gemini-api/docs/openai
external_client = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

model = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

set_default_openai_client(client=external_client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner


async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
        model=model
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):-----------------(ResponseTextDeltaEvent: event data ky class ki type bhi hogi yh type h 
data ki jo wo dega,raw_response_event:loop lgaya h jo us betahasha chizain arhi hn to hamne kaha raw response event jb hoga us wqt wo kam complete krke data return krta h)
            print(event.data.delta, end="", flush=True)-----------(event.data.delta: yh text nikalyga jitna complete hua h
 ( end=""----: by default print jb horha hota h to wo \n se next line per ajata h to hamne kehdia ky next line ni chaiye jo arha h usy dusry ky agay likhty jao space dal kr

asyncio.run(main())



------In Python, flush=True in the print() function forces the output to be written to the console immediately. Normally Python buffers output, which means it might delay displaying 
what you printed until the buffer is full or a newline character is encountered. Using flush=True is particularly useful in scenarios like real-time logging or when printing streaming
data, ensuring that every piece of output is displayed as soon as it's generated.

For example, in your code:
print(event.data.delta.end="", flush=True)

end="": This means no newline character is appended at the end of the printed text.
flush=True: This forces the output to be flushed to the console immediately, even without a newline

------------------------FLUSH=TURE IN OUR CODE JO UPR GYA H------------------------
ğŸ§  Meaning of flush=True
Normally, Python buffers printed text â€” meaning it stores it temporarily before actually showing it on the screen.
So when you print inside a loop, the text might not appear immediately.
âœ… flush=True forces Python to immediately push (flush) the text from the buffer to the console or output stream.

ğŸ’¬ In this context (streaming output)
Here, the model sends chunks of text as it generates them (event.data.delta = partial text).
To make the output appear live, like real-time typing, we use:
flush=True

That way, each small piece (token or word) appears instantly instead of waiting until the model finishes.

âš™ï¸ Example to visualize it
Without flush:
# Text appears all at once at the end
print("H", end="")
print("e", end="")
print("l", end="")
print("l", end="")
print("o", end="")

â†’ Output might show â€œHelloâ€ only after the loop ends.
With flush:
# Text appears letter by letter
print("H", end="", flush=True)
print("e", end="", flush=True)
print("l", end="", flush=True)
print("l", end="", flush=True)
print("o", end="", flush=True)

â†’ Output shows H e l l o in real-time.

ğŸ§¾ Summary
flush=True â†’ forces the output to appear immediately.
Used in streaming to show model text live as itâ€™s generated.
Without it, the console might delay or group text output.

âœ… In short:
flush=True makes the text appear instantly during streaming â€” creating that smooth, â€œtyping liveâ€ effect.




----------------------stream ky ilawa event bhi perform horhy hoty h kunke agent ko hamne task dia to wo bht sary kaam krrha hota ha tool ko cl bhi krta h uska output bhi lata h to bht
sary event perform horhy hoty hn hamne upr waly code m to bs ek event nikala tha jis m wo text complete krke return krta ha to kon kon se event perform hoty hn agr unhy dekhna chahain to?
same steps wohi hn bs thori changes hn jo yh nichy waly code m hui h-----------------------------------------------------------

import asyncio
import random

from agents import Agent, ItemHelpers, Runner, function_tool


@function_tool--------(yh tool decorator function use kia h function ko tool bnadeta h yh)
def how_many_jokes() -> int:------------(yh normal function h jo random number generate krta ha)
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],-------------(jo tool bnaya tha upr wo hamne yhn apne agent ko provide krdia)
        model=model
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",

    )
    print("=== Run starting ===")
    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types





asyncio.run(main())

print("=== Run complete ===")


---------------------chainlit+stream runner in currsor--------------------------------------
import chainlit as cl
from agents import Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel
from agents import function_tool
import os
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)





@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the Panadversity Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    msg = cl.Message(content="")
    await msg.send()
    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = Runner.run_streamed(
        agent, 
        input=history,
        run_config = config,
    )
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            token = event.data.delta
            await msg.stream_token(token)

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)
   
 

--------------------------------------------------------------------------------------------
-------------------IMPORTANT:
Thatâ€™s one of the 3 main ways to run an agent in the Agents SDK:

run_sync â†’ normal, blocking
run_async â†’ async, non-blocking
run_streamed â†’ streaming, real-time output


âš¡ What is â€œstreamingâ€?
Streaming means:
The model starts sending partial output while itâ€™s still generating the rest.
So instead of waiting for the whole response (like sync/async do), your code receives chunks (events) of the output as soon as theyâ€™re available.

Think of it like:
Non-streamed (sync/async): Wait till the whole essay is done â†’ print once.
Streamed: Show words as theyâ€™re typed â†’ print gradually, real-time.

ğŸ§  Runner.run_streamed() â€” what it does
It starts an asynchronous run.
Immediately returns a streaming result object (not the final text).
That result object emits events as the model generates output.

---------------You can then do:CODE-----------------

async for e in result.stream_events():
    print(e)

â†’ This listens for events like tokens, messages, or tool calls as they happen.
This is perfect for live chat UIs (like Chainlit, Gradio, or Colab outputs).

------------------------------------------------------------------------------

ğŸ”„ Runner.run_async() â€” what it does
Runs the same logic but waits until the full response is ready before returning.
You can await it to get the entire message result directly.
No live token-by-token updates.

Example:

result = await Runner.run_async(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you just need the full answer, not a live stream.
--------------------------------------------------------------------------------------

â³ Runner.run_sync() â€” what it does
Same as run_async but for non-async (normal) Python code.
Blocks until the full run completes (like a normal function).

Example:

result = Runner.run_sync(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: youâ€™re in a plain Python script (no asyncio).




---------------------------------------------------------------------
ğŸ’¬ In your code:

result = Runner.run_streamed(agent, "Tell me about recursion...", run_config=config)

Starts the agent.
Doesnâ€™t wait for full completion.
Gives you a stream handle in result.

Then this part:

async for e in result.stream_events():
    print(e)

prints the tokens or partial responses live, as they arrive.
Thatâ€™s how you get â€œlive typingâ€ or real-time generation.


---------------------------------------------
ğŸ§  What is Runner?
Runner is a controller class in the OpenAI Agents SDK thatâ€™s responsible for executing (running) an agent.
Think of it as the engine that takes:
your Agent (the brain),
a user input (the question),
a RunConfig (the setup),
and actually runs the full conversation or reasoning process.

âš™ï¸ In short:
Runner = the component that manages how an agentâ€™s logic is executed â€” including tool calls, message handling, and LLM requests.
---runner jo h wo client to server communication krwata ha or communication sync hogi asyn hogi ya streamed hogi yh define krty.


---------------------------------------
ğŸ§± Analogy â€” AI system as a car
Agent â†’ The driver â€” knows what to do and how to respond.
RunConfig â†’ The GPS/settings â€” defines which model, API key, and configuration to use.
Runner â†’ The engine â€” actually runs the process and makes the agent work.
Without the Runner, the agent just exists but canâ€™t perform any actions.

ğŸ§° Runnerâ€™s Main Methods
run_sync() â†’ Runs the agent normally (blocking execution until done).
run_async() â†’ Runs the agent asynchronously using await (non-blocking).
run_streamed() â†’ Runs the agent in streaming mode, giving partial outputs live while generating.







---------------------TOOL CALLING------------------------------
In AI, a tool is an external helper function or service that an AI model can use to perform specific tasks it canâ€™t do on its own â€” like checking the weather, running code, searching
the web, or doing calculations.
The process of tool calling means the AI automatically decides when and how to use these tools during a conversation.
Itâ€™s like the AI saying:
â€œI canâ€™t do this myself, but I know a tool that can!â€
Then it calls the tool, gets the result, and uses it to respond naturally.

ğŸ§­ Why Tools Are Important
ğŸ•’ Fetch live data â€” e.g., current weather, stock prices, or news.
âš™ï¸ Perform actions â€” e.g., send an email, run Python code, access a database.
âœ… Improve accuracy and safety â€” by verifying answers using reliable sources.

ğŸ“ Summary (in points)
A tool is a helper function that performs one specific task.
Tool calling is when the AI uses that helper automatically during chat.
Tools extend what AI can do (fetch real data, act, or validate info).
The AI uses tools just like a person uses a calculator â€” decides when to use it, gets a result, and explains it.
Tools make AI more useful, interactive, and reliable.


----------------------------------------simple tools code in google colab------------------------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")


# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

#making tool here
from agents.tool import function_tool

@function_tool("get_weather")
def get_weather(location: str, unit: str = "C") -> str:
    """
    fetch the weather for a given location, returning a short description.

    # Example Logic
    """
    return f"The weather in {location} is 22 degrees {unit}."


@function_tool("puaic_student_finder")
def student_finder(student_roll: int) -> str:
    """
    find the PUAIC student based on the roll number """

    Data = {1: "Basim", 2: "Sir Zia", 3: "Amaan"}
    return data.get(student_roll,"not found")


import asyncio
from agents import Agent,Runner

async def main():
  agent = Agent(
      name = "assistant",
      instructions="you are help assistant",
      tools = [get_weather,student_finder],
      model = llm_model
  )

  result = await Runner.run(agent,"call get_weather tool and tell me weather of lahore")
  print(result.final_output)


if __name__ == "__main__":
    asyncio.run(main())

---------------------------------------------------------------------------------------------------
--------------------------------------code of chainlit+function tool in curosr--------------------------------------------

import chainlit as cl
from agents import(Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, 
set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel)
from agents import function_tool
import os
from agents.tool import function_tool

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

@function_tool("get_weather")
def get_weather(location: str, unit: str = "C") -> str:
    """
    fetch the weather for a given location, returning a short description.

    # Example Logic
    """
    return f"The weather in {location} is 22 degrees {unit}."

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
    tools = [get_weather],
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)



@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the your Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    msg = cl.Message(content="")
    await msg.send()
    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = Runner.run_streamed(
        agent, 
        input=history,
        run_config = config,
    )
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            token = event.data.delta
            await msg.stream_token(token)

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)

----------openai agent sdk---------
The Agents SDK is designed with two main goals in mind:

Enough features to be worth using, but few enough primitives to make it quick to learn.(In simple terms:
Primitives = the most basic tools, functions, or components you need to work with the SDK.
Having few primitives means you donâ€™t have to learn many things â€” just a few core ideas that can be combined or extended to build complex behavior.
ğŸ‘‰ Example: Instead of giving you 20 complicated functions, the SDK gives you 3 simple ones that can do everything â€” making it faster and easier to learn.)
Simplicity with power â€“ It includes just the right amount of features to be useful without being overwhelming. You can learn it quickly because it focuses on a few key, essential
tools instead of too many complex ones.

Flexibility â€“ It works perfectly as soon as you install it (â€œout of the boxâ€), but you also have full control to customize behaviors and logic as needed for your application.

ğŸ‘‰ In short:
The Agents SDK is built to be easy to learn, powerful to use, and flexible to customize.


------------------------------------------ğŸ§  Main Features Explained

Agent Loop
The SDK automatically manages the process of calling tools, sending their results to the AI model, and repeating this cycle until the task is finished.
âœ… In short: It handles the full â€œthink â†’ act â†’ reflectâ€ loop for you.

Python-first(python ki hi chizain istemal krty huy new chizain bnyngy jese ek decorator lgaya to graph bn gya is trh sb kch , kch bhi new ni sikhna parega)
You can use normal Python syntax and features to control and connect agents.
âœ… In short: No need to learn new programming styles â€” it works naturally with Python.
Orchestrate = to coordinate or arrange different parts to work together smoothly (like a conductor leading an orchestra).

Handoffs
Lets one agent delegate (pass) a task to another specialized agent.
âœ… In short: Agents can work together and share tasks.

Guardrails
These are safety checks or validations that run alongside your agentâ€™s logic. If something looks wrong, it can stop the process early.
âœ… In short: Ensures inputs and outputs are valid and safe.

Sessions
Automatically keeps conversation history so your agent remembers past context without manual coding.
âœ… In short: Handles memory for you.

Function Tools
Instantly turn any Python function into a usable tool.
The SDK automatically creates input/output schemas (data formats) and validates them with Pydantic (a Python library for data validation).
âœ… In short: Makes your Python functions ready for use by the agent with no extra effort.

Tracing
Provides visual tracking and debugging of agent workflows.
You can also use OpenAI tools for evaluation, fine-tuning, and distillation (improving models).
âœ… In short: Lets you see whatâ€™s happening inside and improve performance.

ğŸª„ Overall Summary
The Agents SDK gives developers powerful tools to build AI agents easily in Python.
It manages loops, memory, safety, and collaboration automatically â€” while letting you monitor, debug, and extend behavior smoothly.

ğŸ‘‰ In one line:
The SDK helps you build, control, and monitor AI agents effortlessly using simple Python code.



----------------------------------
---generic typehints deta h



------------In Python, @dataclass is a decorator that automatically adds common methods to a class â€” like __init__(), __repr__(), and __eq__() â€” so you donâ€™t have to write them manually.

ğŸ§© Meaning:
A decorator (like @dataclass) modifies how a class or function behaves â€” it adds extra functionality.

ğŸ§  Why use @dataclass:
Itâ€™s used to create data-holding classes (like records or structs) more easily.

Without @dataclass ğŸ‘‡

class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age


With @dataclass ğŸ‘‡

from dataclasses import dataclass

@dataclass
class Person:
    name: str
    age: int


âœ… Now Python automatically creates:
__init__() â€“ constructor
__repr__() â€“ readable string output
__eq__() â€“ equality check

ğŸª„ In short:
@dataclass makes it faster and cleaner to define simple classes that store data â€” no need to write repetitive code.


---------------------------------------DATACLASS AND PYDANTIC----------------------------
Purpose
dataclass is used to create simple data containers with less code.
Pydantic is used to validate and parse data (especially from external sources like APIs or JSON).

Type Checking
dataclass does not enforce types at runtime â€” type hints are just for readability.
Pydantic checks and enforces types at runtime, converting or rejecting invalid data.

Validation
dataclass has no built-in validation. You must write custom checks.
Pydantic automatically validates inputs and raises errors if data is incorrect.

Automatic Conversion
dataclass wonâ€™t convert "25" (string) to 25 (int).
Pydantic automatically converts compatible types.

Error Handling
dataclass wonâ€™t stop you from passing wrong types.
Pydantic raises detailed validation errors.

Performance
dataclass is faster and lighter (no validation overhead).
Pydantic is slightly slower because it performs runtime validation.

Use Case
Use dataclass for internal data storage or simple models.
Use Pydantic for external data, user input, or API responses that need validation.

ğŸ‘‰ In short:
dataclass = simple and lightweight.
Pydantic = powerful and safe with validation.

-------------data class built-in h or pydantic ki library import install krni parti h
---dataclass m bhi methods bna skty hn  or class method or class variable bhi define krskty hn dataclass m


----------An instance variable is a variable that belongs to a specific object (instance) of a class in Python.
ğŸ§© Explanation:
Each time you create an object from a class, it gets its own copy of the instance variables.
These variables are usually defined inside the __init__() method using self.

-------------------------------------ğŸ§° Example:---------------------------------
class Car:
    def __init__(self, brand, color):
        self.brand = brand      # instance variable
        self.color = color      # instance variable

# creating two objects
car1 = Car("Toyota", "Red")
car2 = Car("Honda", "Blue")

print(car1.brand)  # Toyota
print(car2.brand)  # Honda


Here:
brand and color are instance variables.
car1 and car2 each have their own separate values for these variables.

ğŸª„ In short:
An instance variable stores data unique to each object created from a class.


---------class method ko class ky sth call krskty hn
--------instance method keliye class ka phly ham instance bnaty hn or instance bnany ky bd phr ham usy use krhy hoty hn





---------------------------instance vs class
ek h class level or ek h object(instance) level
ab ham masjid jaty hn whn namaz shuru hony ka time sbke liye same h to masjid jo h wo class(class level per) h or us m hr bnda jo ha(object , instance) uske pas apni apni watch h us per
kisi ka time ek mnt agay h kisi ka fix h to yh ha object level


------------------------------------------------------------------
ğŸ§  1. Instance Variable
Belongs to a specific object (instance) of a class.
Defined using self inside __init__().
Each object has its own copy of these variables.

âœ… Example:

class Car:
    def __init__(self, brand):
        self.brand = brand  # instance variable

ğŸ·ï¸ 2. Class Variable
Shared by all objects of the class.
Defined outside any method, usually directly under the class.
Changing it affects all instances (unless overridden).

âœ… Example:

class Car:
    wheels = 4  # class variable
    def __init__(self, brand):
        self.brand = brand  # instance variable

âš™ï¸ 3. Instance Method
Works with instance variables.
Needs access to the object (self).
Used to define behavior for individual objects.

âœ… Example:

class Car:
    def start(self):  # instance method
        print("Car started")

ğŸ§© 4. Class Method
Works with class variables, not instance variables.
Defined using the @classmethod decorator.
Takes cls (class reference) instead of self.

âœ… Example:

class Car:
    wheels = 4

    @classmethod
    def change_wheels(cls, new_count):
        cls.wheels = new_count

----------------------------------------
ğŸ”¹ Instance Variable
Belongs to one specific object.
Defined inside __init__() using self.
Each object has its own copy.

ğŸ”¹ Class Variable
Shared by all objects of the class.
Defined outside any method (inside the class).
Changing it affects all instances.

ğŸ”¹ Instance Method
Works with instance variables.
Takes self as the first parameter.
Defines behavior for individual objects.

ğŸ”¹ Class Method
Works with class variables.
Takes cls as the first parameter.
Defined using @classmethod.
Affects the entire class, not just one object.


ğŸ‘‰ Simple Summary:
Instance variable/method â†’ specific to each object.
Class variable/method â†’ shared by all objects.



----------------------------dataclass code run both in cursor or colab-------------------------------

from dataclasses import dataclass
from typing import TypeVar,ClassVar

@dataclass         #decorator h
class American:                      #same as "class American(object):" yh (object) dalny ki zrort ni hoti kunke compiler khud dal deta h actually interpreter
  name: str
  age: int
  weight: float
  liked_food: str
  national_language: ClassVar[str] = "english"      #[str] is called generics
  national_food : ClassVar[str] = "Hamburger"           
  normal_body_temperatur: ClassVar[float] = 98.6

  def speaks(self):         #instance funcction bn gya
    return f"{self.name} is speaking {American.national_language}"

  def eats(self):
    return f"{self.name} is eating"

  @staticmethod               #( yh class ka static method h to yh srf class ky zariye hi call hoskta h,yh instance(object) se belong ni krta)
  def country_language():
    return American.national_language

    
john = American(name="isha",age=27,weight=67.5)          #(hr object ka apna naam apna age weight or language hoga or wo do method hongy speaks or eats jo unka individual hoga yh
ek object tha john asy or bhi bnaskty bob,alice or bhi bht sary unke pas bhi 4 variable or 2 methods individual hongy)
print(john.speaks())
print(john.eats())
print(American.country_language())

print(john.name)
print(john.age)
print(john.weight)
print(john)
print(American.national_language)



---------------class ka object bnarhy hn to usy instancetiate krrhy hhoty hn usy callable ni bolengy


---------------------------------public , protected and private variable-------------------------------------
ğŸ”¹ Protected Variables
Named with a single underscore _variable.
Means: â€œfor internal use onlyâ€ (not enforced, just a convention).
Can still be accessed from outside the class if needed.

âœ… Example:

from dataclasses import dataclass

@dataclass
class Car:
    _speed: int  # protected variable

car = Car(100)
print(car._speed)  # allowed, but not recommended


ğŸ”¹ Private Variables
Named with double underscores __variable.
Python automatically name-mangles it (changes the name internally).
Not directly accessible from outside the class.

âœ… Example:

from dataclasses import dataclass

@dataclass
class Car:
    __engine_number: str  # private variable

car = Car("EN123")
# print(car.__engine_number)  âŒ Error
print(car._Car__engine_number)  # âœ… Works (name mangling)


ğŸ”¹ Public Variables in Dataclass
Variables without underscores are public.
They can be freely accessed and modified from anywhere.
This is the default in Python (and in dataclasses).


âœ… Example:
from dataclasses import dataclass

@dataclass
class Car:
    brand: str      # public
    model: str      # public

car = Car("Toyota", "Corolla")
print(car.brand)   # âœ… Accessible
car.model = "Yaris"  # âœ… Can modify


ğŸª„ In short:
brand â†’ Public (default, fully accessible)
_speed â†’ Protected (for internal use)
__engine_number â†’ Private (hidden from outside)


ğŸª„ In short:
_var â†’ Protected (accessible but meant for internal use).
__var â†’ Private (hidden via name mangling).
Works the same in dataclasses as in normal Python classes â€” the decorator doesnâ€™t change this behavior.



---------------static variables static method m bhi available hoty hn or instance methods m bhi available hoty h
---------class variable available hoty hn sary objects m or unka sary objects m ek hi value hoti h


---------system prompt wo instruction jo ham llm ko pass krrhy hn ky yh tumhara persona h yh goal h yh type h
----instruction ya to string hogi ya callable hogi ya none hogi or default m equals to none hogi
1.string(str) hamne llm ko pas kia or llm ne isy instruction ky tor per lia or llm ne response krdia phr
2.dusra instruction callable bhi hoskta h mtlb ham dynamically koi bhi chez agent ya llm ko pass krskta hu instructions ky tor per(koi bhi function hoskta ha jo kch kam krny ky bd
dynamically system prompt hamy return krwata hoga string ky andr or wo pass hojyga llm ko)
3.none mtlb koi bhi instruction pas ni krrhy none pas krrhy



---------------------------str,callable,none-----------------------------------
This means that the instruction (a variable or parameter) can accept three possible types of values:

ğŸ”¹ 1. str (String)
It can be a text instruction â€” a normal string.
âœ… Example:
instruction = "Start the process"

ğŸ”¹ 2. callable
It can be a function or method that can be called/executed.
âœ… Example:

def greet():
    print("Hello!")

instruction = greet  # callable
instruction()        # runs the function


Meaning: You can pass a function itself instead of text, so it can be executed later.

ğŸ”¹ 3. None
It means no instruction is given (empty or inactive).
âœ… Example:
instruction = None

ğŸª„ In short:
instruction can be:
A string â†’ simple message or text command.
A callable â†’ a function that can run.
None â†’ no instruction at all.



---------------------------CALLABLE---------------------------
--------------------ğŸ§¸ Example 1 â€” Function

def say_hello():
    print("Hello!")

say_hello()   # You called it! It says Hello!


ğŸ’¡ The function is callable because you can call it with ().




----------------------ğŸš— Example 2 â€” Class with a method
class Car:
    def start(self):
        print("Vroom!")

my_car = Car()
my_car.start()  # You called the method â€” the car starts!


ğŸ’¡ my_car.start is callable because you can use () to make it do something.



---------------ğŸ§© Example 3 â€” A callable object

You can even make an object callable by giving it a special power called __call__.

class Greeter:
    def __call__(self):
        print("Hi there!")

g = Greeter()
g()  # works like a function!


ğŸ’¡ g is not a function â€” itâ€™s an object â€” but you can still â€œcallâ€ it. Thatâ€™s why itâ€™s callable!



-------------------------âŒ Example 4 â€” Not callable
toy = "Teddy bear"
toy()  # âŒ Error! Strings canâ€™t be called


ğŸ’¡ You canâ€™t â€œcallâ€ a string â€” itâ€™s not callable.



--------------------------------------------ğŸª„ Check if something is callable
print(callable(say_hello))  # True
print(callable(toy))        # False

------------ğŸŒŸ In short:
Callable = something you can call with ()
Examples: functions, methods, or special objects.
If you canâ€™t â€œcallâ€ it, Python gives an error.


----------ğŸ§ Think of it like this:
A callable is like a button you can press (with ()) to make something happen!


-------------------------
def greet():
  return "hello"

print(greet())
dir(greet())      #dir jo ha wo greet() function ki puri directory dikha dega
#dir(greet) â†’ lists everything Python stores inside the function object, like its built-in properties.

output:
hello
['__add__',
 '__class__',
 '__contains__',
 '__delattr__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getitem__',
 '__getnewargs__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__iter__',
 '__le__',
 '__len__',
 '__lt__',
 '__mod__',
 '__mul__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__rmod__',
 '__rmul__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 'capitalize',
 'casefold',
 'center',
 'count',
 'encode',
 'endswith',
 'expandtabs',
 'find',
 'format',
 'format_map',
 'index',
 'isalnum',
 'isalpha',
 'isascii',
 'isdecimal',
 'isdigit',
 'isidentifier',
 'islower',
 'isnumeric',
 'isprintable',
 'isspace',
 'istitle',
 'isupper',
 'join',
 'ljust',
 'lower',
 'lstrip',
 'maketrans',
 'partition',
 'removeprefix',
 'removesuffix',
 'replace',
 'rfind',
 'rindex',
 'rjust',
 'rpartition',
 'rsplit',
 'rstrip',
 'split',
 'splitlines',
 'startswith',
 'strip',
 'swapcase',
 'title',
 'translate',
 'upper',
 'zfill']



-----------------------------------callable vs non-callable object------------------------
--------NON-CALLABLE OBJECT CODE----------------
from dataclasses import dataclass

@dataclass
class Human:
    # Attributes
    name: str
    age: int

    # Method to greet
    def greet(self):
        return f"HI, I'm {self.name}"

    # Another method
    def works(self):
        return "I am working"

# Object instantiation (partially visible in the screenshot)
obj1 = Human(name="John", age=22)

print(obj1.name)         #OUTPUT: John
print(obj1.age)          #OUTPUT: 22
print(obj1.greet())      #OUTPUT:HI,I'm John
print(obj1.works())      #OUTPUT:I am working
obj1()                   #IT GIVES ERROR:---------------------------------------------------------------------------
                        TypeError                                 Traceback (most recent call last)
                        /tmp/ipython-input-1288370025.py in <cell line: 0>()
                                ----> 1 obj1()
                            TypeError: 'Human' object is not callable




---------------------------CALLABLE OBJECT CODE----------------------------------
from dataclasses import dataclass

@dataclass
class Human:
    # Attributes
    name: str
    age: int

    # Method to greet
    def greet(self):
        return f"HI, I'm {self.name}"

    # Another method
    def works(self):
        return "I am working"

    def __call__(self):          #yh magic method bhi kehty hn or dunder function bhi dunder means double underscore to isy jo hamara object h wo callable hojyga or output ajana
      return "hello"
      
# Object instantiation (partially visible in the screenshot)
obj1 = Human(name="John", age=22)

print(obj1.name)         #OUTPUT: John
print(obj1.age)          #OUTPUT: 22
print(obj1.greet())      #OUTPUT:HI,I'm John
print(obj1.works())      #OUTPUT:I am working
obj1()                   OUTPUT: 'hello'



----------------bydefault sary function dunder hoty hn callable hoty hn lekin jo instance hota h means object to wo callable ni hota usko callable bnana hamne upr dekha
-----class m () ky andr self ni lgaty ya zrori ni baqi sb function m lgaty self () is m or self jo h wo class ka refrence h ky class ky andr ek method h to usy call krny keliye self
dengy mtlb yh isi class ka h self h yh khud h 





-------------------------------------------SELF UNDERSTANDING----------------------
-----------------;;;;;;;;;;;;;;;;;;;QUESTION;;;;;;;;;;;;;;;;;;;;;;;;;---------------------
@dataclass
class Human:
    # Attributes
    name: str
    age: int

    # Method to greet
    def greet(self):
        return f"HI, I'm {self.name}"

    # Another method
    def works(self):
        return "I am working"
----( in this code in greet method we are passing self and in class Human we are not passing self why? and what does self means? what is refer?


---------------------------------;;;;;;;;;;;;;;;;ANSWER;;;;;;;;;;;------------------
ğŸ§  Step 1: Understanding what a class really is

A class is just a blueprint â€” it tells Python what data (attributes) and what actions (methods) an object will have.

Example:

@dataclass
class Human:
    name: str
    age: int


Here, we are only defining the structure.
Weâ€™re saying: â€œEvery Human will have a name and an age.â€
So we donâ€™t need to pass self here â€” because this part just describes what a Human looks like, not what it does.

ğŸ§© Step 2: When we define a method inside the class
Now look at:

def greet(self):
    return f"Hi, I'm {self.name}"

Here we are defining an action â€” something a Human object can do.
So, when you create an object:

person = Human("Queen", 20)

and then call:

person.greet()


Python secretly calls it like this:

Human.greet(person)


That means the object person (which is Human("Queen", 20)) is automatically passed to the function as the first argument â€” and thatâ€™s what self refers to.

ğŸ’¡ Step 3: What self means

ğŸ‘‰ self simply means â€œthis objectâ€.
It refers to the specific object that is calling the method.

So when you write:

return f"Hi, I'm {self.name}"


self.name means â€” â€œget the name value that belongs to this particular Human object.â€

ğŸª„ Step 4: Why not in the class header?
Because the class header defines the blueprint (the design), not an instance (object).
self only exists after you create an object from the class.

-----------------ğŸ’¡ Summary:
self means â€œthis objectâ€ â€” it refers to the object that calls the method.
You write self as the first parameter in methods inside a class.
When you call a method like person.greet(), Python secretly does Human.greet(person).
Thatâ€™s why the object (person) is automatically passed as self.
self lets you access object attributes (like self.name or self.age).
In the class header, you donâ€™t write self because youâ€™re defining a blueprint, not a specific object.
self is only used inside methods that work with individual objects.



-------------QUESTION:what is diffrence between def __init__ or def __call__ and why we dont make def __init__ in dataclass and we make def __init__ it in normal class?----------
--------------------------------------------------ANSWER--------------------------------------
ğŸ’¡ 1. __init__ vs __call__
ğŸ§© __init__:

Itâ€™s the initializer (constructor) method of a class.
It runs automatically when you create an object.
Its job is to set up the objectâ€™s attributes.

Example:

class Human:
    def __init__(self, name, age):
        self.name = name
        self.age = age


Now:
person = Human("Queen", 20)

Here, Python automatically calls __init__(person, "Queen", 20).


ğŸª„ __call__:
It makes an object itself callable like a function.
If a class defines __call__, you can do object() as if it were a function.

Example:

class Greet:
    def __call__(self):
        return "Hello!"

g = Greet()
print(g())  # Notice the parentheses! â†’ Calls __call__

So __call__ lets an object behave like a function.

ğŸ’« 2. Why we donâ€™t write __init__ in a dataclass
When you use @dataclass, Python automatically creates the __init__ method for you!

Example:

from dataclasses import dataclass

@dataclass
class Human:
    name: str
    age: int


is the same as writing:

class Human:
    def __init__(self, name, age):
        self.name = name
        self.age = age


So, dataclasses save you from boilerplate code â€” you donâ€™t need to manually write __init__, __repr__, or __eq__ unless you want to customize them.

ğŸ§  Summary in Points
__init__ â†’ sets up object attributes when object is created.
__call__ â†’ allows an object to be called like a function.
Dataclasses automatically generate __init__ for you.
Normal classes need you to manually define __init__.
You can still define your own __init__ in a dataclass if you need custom setup.





--------------------QUESTION:WHEN me make dataclass so and make class then we make object of it so whats basically the scenario that connect it all together how object is making
here with dataclass-------------------------------------
---------------------------------------------------------------------ANSWER-----------------------------------------
ğŸŒ¸ 1. Normal class â€” what happens when you make an object
When you create a normal class, you must define your own __init__ (constructor):

class Human:
    def __init__(self, name, age):
        self.name = name
        self.age = age


Now when you write:

person = Human("Queen", 20)


Python does this internally:
Creates empty space in memory for person (a Human object).
Calls __init__(person, "Queen", 20).
Fills in the data â†’ person.name = "Queen" and person.age = 20.

ğŸ’« 2. Dataclass â€” what changes
When you use @dataclass, Python automatically creates that __init__ method for you.

from dataclasses import dataclass

@dataclass
class Human:
    name: str
    age: int


You donâ€™t see it, but Python secretly generates this behind the scenes:

def __init__(self, name: str, age: int):
    self.name = name
    self.age = age


So when you write:

person = Human("Queen", 20)


The same exact process happens:
Memory is allocated for person.
The auto-generated __init__ runs.
It stores name and age inside that object.

ğŸ§© 3. So whatâ€™s connecting it all together?
Hereâ€™s the chain of connection in simple steps:
@dataclass tells Python â†’ â€œPlease create extra magic methods for this class.â€
One of those methods is __init__.
When you call Human("Queen", 20), Python automatically calls that generated __init__.
That __init__ puts your values into the object (self.name, self.age).
You now have a fully formed object (person) with data inside it.

ğŸ’¡ 4. In short:
@dataclass = a decorator that auto-creates code (like __init__, __repr__, __eq__).
When you create an object, the auto-generated __init__ runs behind the scenes.
So the object creation flow stays exactly the same, only with less code to write.



---------------------------------QUESTION : __new__ vs __init__ ------------------------------------------------------
---------------------------------------ANSWER------------------------------------------
ğŸ’¡ Difference between __new__ and __init__ in Python
ğŸ§© 1. __new__ â†’ creates the object
ğŸª„ 2. __init__ â†’ initializes the object

They both work together, but do different jobs.

ğŸ§± Step-by-step example
When you write:

person = Human("Queen", 20)

Python does this under the hood:

Calls Human.__new__(Human) â†’ this actually creates the empty object in memory.

Then automatically calls Human.__init__(person, "Queen", 20) â†’ this fills in the data (sets attributes).

So:
__new__ = â€œmake me an empty boxâ€
__init__ = â€œfill that box with stuffâ€

ğŸ§  Example
class Human:
    def __new__(cls):
        print("Step 1: Creating object (memory allocated)")
        instance = super().__new__(cls)
        return instance

    def __init__(self):
        print("Step 2: Initializing object (attributes set)")


Output:

Step 1: Creating object (memory allocated)
Step 2: Initializing object (attributes set)

ğŸª„ Simple Analogy
Imagine youâ€™re baking a cake ğŸ°
__new__ â†’ builds the empty cake base (the sponge).
__init__ â†’ decorates it (adds cream, frosting, etc).
You canâ€™t decorate before you bake â€” same logic!

ğŸ’¬ Why we rarely use __new__
You almost never need to define __new__ in normal classes or dataclasses.
Itâ€™s mainly used when creating immutable objects (like tuples, strings) or customizing metaclass behavior.
In regular classes, we only define __init__.

ğŸŒ¸ Summary in Points
__new__ â†’ creates the object (allocates memory).
__init__ â†’ initializes the object (sets its attributes).
Python always calls __new__ first, then __init__.
You usually define only __init__; __new__ is for advanced use.
In dataclasses, __init__ is auto-generated; __new__ is still used internally by Python.



-----------run_sync method static h runner ki class m 




---------------------RSI----------------------
In AI, RSI usually stands for Recursive Self-Improvement.

Hereâ€™s a quick breakdown ğŸ‘‡
ğŸ¤– RSI (Recursive Self-Improvement) in AI

Meaning:
Itâ€™s when an artificial intelligence system improves its own design or capabilities â€” and each improved version becomes better at improving itself.
In simple words:

The AI keeps making itself smarter, faster, and more efficient â€” on its own.

âš™ï¸ How it works (conceptually)
AI learns to modify its own algorithms or parameters.
Creates a better version of itself.
The new version repeats the process â€” improving even faster.
This can lead to an â€œintelligence explosionâ€, where AI rapidly surpasses human intelligence.

ğŸŒ Example idea
An AI starts by optimizing its neural network structure.
It learns better data-processing methods.
It rewrites parts of its own code for efficiency.
Each cycle = smarter AI than before.

âš ï¸ Risks & Discussion
Uncontrolled self-improvement might make AI unpredictable.
Itâ€™s a major topic in AI safety and ethics research.
The concept ties closely to Artificial General Intelligence (AGI).





































