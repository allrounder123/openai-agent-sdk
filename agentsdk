
-----------agent jo h wo autonomous action lene wala ek model h--------------------------
------async mtlb ham request bhjty hn wo process krta h or jb uska process khtm hoga wo response dedega-------------------------
--------openai ky sdk ko use krty huy ham apny customize llm ko use krty hn jese openai m gemini flash use krty-----------------sdk agent or sary ky sary features sdk ky hongy lekin 
backend per jo llm hoga wo free wala hoga gemini ka-------------demagh jo h wo gemini ka or jo decision making wagaira h wo openai sdk ka h
-----------sync mtlb task jb perform horha h ussi wqt response do , or async mtlb 10 15 mnt bhi sochlo or jb response complete hojye tb response dedo
--------Runner m bht sary function method hoty by default async hota or ham Runner.run_sync krty hn (run_sync method h Runner m)(client to server communication jo krwaty hn wo sary 
tareeky runner m mojud hn)
------sync mtlb mene apko kaha biryani lekr ao to jb tk ap biryani ni laogy tb tk m wait krungi kisi  se bt ni krungi intezar krungi or async mtlb mene kaha ap biryani lekr ao or jb tk ap
ni aty tb tk m baqi kaam nipta lungi bt krlungi sbse kaam krlungi




-----------------------
In the OpenAI Agents SDK (and similar frameworks), you‚Äôll often see three levels of logic:
Global level ‚Üí Agent level ‚Üí Run level

Let‚Äôs break each down simply üëá
üåç Global Level
The topmost layer ‚Äî things that apply to the whole app or system, not just one agent or one conversation.
It defines shared settings, global tools, and event handlers that affect everything.

Think of it as:
üß† ‚ÄúWhat rules or tools should all agents or runs in my app have access to?‚Äù

Examples:
Registering a tool that every agent can use (like a database connector).
Defining logging or error-handling behavior for the whole app.
Setting up startup events, environment variables, or authentication.

üß© Agent Level
Defines the behavior and configuration of one specific agent.
Example: system prompt, tool access, goals, or memory setup.
Agents can inherit or override global settings.

Think of it as:
ü§ñ ‚ÄúWho the agent is and what it can do.‚Äù

‚öôÔ∏è Run Level
The execution layer ‚Äî what happens in a single conversation or task run.
This is where the agent actually processes input, calls tools, and produces outputs.

Think of it as:
üèÉ ‚ÄúWhat happens right now while the agent is running.‚Äù





---------------------SYNC FUNCTION IN RUNNER--------------(WORK ON GOOGLE COLAB)--------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

math_agent: Agent = Agent(name="MathAgent",
                     instructions="You are a helpful math assistant.",
                     model=llm_model) # gemini-2.5 as agent brain - chat completions

result: Runner = Runner.run_sync(math_agent, "why learn math for AI Agents?", run_config=config)

print("\nCALLING AGENT\n")
print(result.final_output)




-------------------------------------------------------------------------------------------------------------------------------------
-------------ASYNC FUNCTION IN RUNNER----------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio(is library se koi bhi async function chla skty hn)

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)---(run time configuration)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = await Runner.run(agent, "Tell me about recursion in programming.", run_config=config)-----------(.run ka method Runner m by-default async hi hota h)( or await mtlb jo kam 
krrha h uska wait krega or jb wo response ajyga usko result m dal dega)
    print(result.final_output)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.

if __name__ = "__main__":(is line ki zrort .py file m hoti h jo vs or cursor m hota h yhn iski zrort ni h kunke google colab m import asyncio krdia h hamne jo async ky sary function chlata ha
asyncio.run(main())



--------------chatgpt foran reply krta h to wo async h agr kisi kam per soch kr phr response kry to wo sync

-------------------STREAMED FUNCTION IN RUNNER-----------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = Runner.run_streamed(agent, "Tell me about recursion in programming.", run_config=config)
    print(result)
    async for e in result.stream_events():
      print(e)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.


asyncio.run(main())


-----------------------------------streamed code run in google colab---------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
import os
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
from agents.run import RunConfig
from google.colab import userdata
import asyncio

from agents import (
    Agent,
    Runner,
    set_default_openai_api,
    set_default_openai_client,
    set_tracing_disabled,
)

gemini_api_key = userdata.get("GEMINI_API_KEY")


# Check if the API key is present; if not, raise an error
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.")

#Reference: https://ai.google.dev/gemini-api/docs/openai
external_client = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

model = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

set_default_openai_client(client=external_client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner


async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
        model=model
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):-----------------(ResponseTextDeltaEvent: event data ky class ki type bhi hogi yh type h 
data ki jo wo dega,raw_response_event:loop lgaya h jo us betahasha chizain arhi hn to hamne kaha raw response event jb hoga us wqt wo kam complete krke data return krta h)
            print(event.data.delta, end="", flush=True)-----------(event.data.delta: yh text nikalyga jitna complete hua h
 ( end=""----: by default print jb horha hota h to wo \n se next line per ajata h to hamne kehdia ky next line ni chaiye jo arha h usy dusry ky agay likhty jao space dal kr

asyncio.run(main())



------In Python, flush=True in the print() function forces the output to be written to the console immediately. Normally Python buffers output, which means it might delay displaying 
what you printed until the buffer is full or a newline character is encountered. Using flush=True is particularly useful in scenarios like real-time logging or when printing streaming
data, ensuring that every piece of output is displayed as soon as it's generated.

For example, in your code:
print(event.data.delta.end="", flush=True)

end="": This means no newline character is appended at the end of the printed text.
flush=True: This forces the output to be flushed to the console immediately, even without a newline

------------------------FLUSH=TURE IN OUR CODE JO UPR GYA H------------------------
üß† Meaning of flush=True
Normally, Python buffers printed text ‚Äî meaning it stores it temporarily before actually showing it on the screen.
So when you print inside a loop, the text might not appear immediately.
‚úÖ flush=True forces Python to immediately push (flush) the text from the buffer to the console or output stream.

üí¨ In this context (streaming output)
Here, the model sends chunks of text as it generates them (event.data.delta = partial text).
To make the output appear live, like real-time typing, we use:
flush=True

That way, each small piece (token or word) appears instantly instead of waiting until the model finishes.

‚öôÔ∏è Example to visualize it
Without flush:
# Text appears all at once at the end
print("H", end="")
print("e", end="")
print("l", end="")
print("l", end="")
print("o", end="")

‚Üí Output might show ‚ÄúHello‚Äù only after the loop ends.
With flush:
# Text appears letter by letter
print("H", end="", flush=True)
print("e", end="", flush=True)
print("l", end="", flush=True)
print("l", end="", flush=True)
print("o", end="", flush=True)

‚Üí Output shows H e l l o in real-time.

üßæ Summary
flush=True ‚Üí forces the output to appear immediately.
Used in streaming to show model text live as it‚Äôs generated.
Without it, the console might delay or group text output.

‚úÖ In short:
flush=True makes the text appear instantly during streaming ‚Äî creating that smooth, ‚Äútyping live‚Äù effect.




----------------------stream ky ilawa event bhi perform horhy hoty h kunke agent ko hamne task dia to wo bht sary kaam krrha hota ha tool ko cl bhi krta h uska output bhi lata h to bht
sary event perform horhy hoty hn hamne upr waly code m to bs ek event nikala tha jis m wo text complete krke return krta ha to kon kon se event perform hoty hn agr unhy dekhna chahain to?
same steps wohi hn bs thori changes hn jo yh nichy waly code m hui h-----------------------------------------------------------

import asyncio
import random

from agents import Agent, ItemHelpers, Runner, function_tool


@function_tool--------(yh tool decorator function use kia h function ko tool bnadeta h yh)
def how_many_jokes() -> int:------------(yh normal function h jo random number generate krta ha)
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],-------------(jo tool bnaya tha upr wo hamne yhn apne agent ko provide krdia)
        model=model
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",

    )
    print("=== Run starting ===")
    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types





asyncio.run(main())

print("=== Run complete ===")


---------------------chainlit+stream runner in currsor--------------------------------------
import chainlit as cl
from agents import Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel
from agents import function_tool
import os
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)





@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the Panadversity Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    msg = cl.Message(content="")
    await msg.send()
    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = Runner.run_streamed(
        agent, 
        input=history,
        run_config = config,
    )
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            token = event.data.delta
            await msg.stream_token(token)

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)
   
 

--------------------------------------------------------------------------------------------
-------------------IMPORTANT:
That‚Äôs one of the 3 main ways to run an agent in the Agents SDK:

run_sync ‚Üí normal, blocking
run_async ‚Üí async, non-blocking
run_streamed ‚Üí streaming, real-time output


‚ö° What is ‚Äústreaming‚Äù?
Streaming means:
The model starts sending partial output while it‚Äôs still generating the rest.
So instead of waiting for the whole response (like sync/async do), your code receives chunks (events) of the output as soon as they‚Äôre available.

Think of it like:
Non-streamed (sync/async): Wait till the whole essay is done ‚Üí print once.
Streamed: Show words as they‚Äôre typed ‚Üí print gradually, real-time.

üß† Runner.run_streamed() ‚Äî what it does
It starts an asynchronous run.
Immediately returns a streaming result object (not the final text).
That result object emits events as the model generates output.

---------------You can then do:CODE-----------------

async for e in result.stream_events():
    print(e)

‚Üí This listens for events like tokens, messages, or tool calls as they happen.
This is perfect for live chat UIs (like Chainlit, Gradio, or Colab outputs).

------------------------------------------------------------------------------

üîÑ Runner.run_async() ‚Äî what it does
Runs the same logic but waits until the full response is ready before returning.
You can await it to get the entire message result directly.
No live token-by-token updates.

Example:

result = await Runner.run_async(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you just need the full answer, not a live stream.
--------------------------------------------------------------------------------------

‚è≥ Runner.run_sync() ‚Äî what it does
Same as run_async but for non-async (normal) Python code.
Blocks until the full run completes (like a normal function).

Example:

result = Runner.run_sync(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you‚Äôre in a plain Python script (no asyncio).




---------------------------------------------------------------------
üí¨ In your code:

result = Runner.run_streamed(agent, "Tell me about recursion...", run_config=config)

Starts the agent.
Doesn‚Äôt wait for full completion.
Gives you a stream handle in result.

Then this part:

async for e in result.stream_events():
    print(e)

prints the tokens or partial responses live, as they arrive.
That‚Äôs how you get ‚Äúlive typing‚Äù or real-time generation.


---------------------------------------------
üß† What is Runner?
Runner is a controller class in the OpenAI Agents SDK that‚Äôs responsible for executing (running) an agent.
Think of it as the engine that takes:
your Agent (the brain),
a user input (the question),
a RunConfig (the setup),
and actually runs the full conversation or reasoning process.

‚öôÔ∏è In short:
Runner = the component that manages how an agent‚Äôs logic is executed ‚Äî including tool calls, message handling, and LLM requests.
---runner jo h wo client to server communication krwata ha or communication sync hogi asyn hogi ya streamed hogi yh define krty.


---------------------------------------
üß± Analogy ‚Äî AI system as a car
Agent ‚Üí The driver ‚Äî knows what to do and how to respond.
RunConfig ‚Üí The GPS/settings ‚Äî defines which model, API key, and configuration to use.
Runner ‚Üí The engine ‚Äî actually runs the process and makes the agent work.
Without the Runner, the agent just exists but can‚Äôt perform any actions.

üß∞ Runner‚Äôs Main Methods
run_sync() ‚Üí Runs the agent normally (blocking execution until done).
run_async() ‚Üí Runs the agent asynchronously using await (non-blocking).
run_streamed() ‚Üí Runs the agent in streaming mode, giving partial outputs live while generating.







---------------------TOOL CALLING------------------------------
In AI, a tool is an external helper function or service that an AI model can use to perform specific tasks it can‚Äôt do on its own ‚Äî like checking the weather, running code, searching
the web, or doing calculations.
The process of tool calling means the AI automatically decides when and how to use these tools during a conversation.
It‚Äôs like the AI saying:
‚ÄúI can‚Äôt do this myself, but I know a tool that can!‚Äù
Then it calls the tool, gets the result, and uses it to respond naturally.

üß≠ Why Tools Are Important
üïí Fetch live data ‚Äî e.g., current weather, stock prices, or news.
‚öôÔ∏è Perform actions ‚Äî e.g., send an email, run Python code, access a database.
‚úÖ Improve accuracy and safety ‚Äî by verifying answers using reliable sources.

üìù Summary (in points)
A tool is a helper function that performs one specific task.
Tool calling is when the AI uses that helper automatically during chat.
Tools extend what AI can do (fetch real data, act, or validate info).
The AI uses tools just like a person uses a calculator ‚Äî decides when to use it, gets a result, and explains it.
Tools make AI more useful, interactive, and reliable.


----------------------------------------simple tools code in google colab------------------------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")


# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

#making tool here
from agents.tool import function_tool

@function_tool("get_weather")
def get_weather(location: str, unit: str = "C") -> str:
    """
    fetch the weather for a given location, returning a short description.

    # Example Logic
    """
    return f"The weather in {location} is 22 degrees {unit}."


@function_tool("puaic_student_finder")
def student_finder(student_roll: int) -> str:
    """
    find the PUAIC student based on the roll number """

    Data = {1: "Basim", 2: "Sir Zia", 3: "Amaan"}
    return data.get(student_roll,"not found")


import asyncio
from agents import Agent,Runner

async def main():
  agent = Agent(
      name = "assistant",
      instructions="you are help assistant",
      tools = [get_weather,student_finder],
      model = llm_model
  )

  result = await Runner.run(agent,"call get_weather tool and tell me weather of lahore")
  print(result.final_output)


if __name__ == "__main__":
    asyncio.run(main())

---------------------------------------------------------------------------------------------------
--------------------------------------code of chainlit+function tool in curosr--------------------------------------------

import chainlit as cl
from agents import(Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, 
set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel)
from agents import function_tool
import os
from agents.tool import function_tool

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

@function_tool("get_weather")
def get_weather(location: str, unit: str = "C") -> str:
    """
    fetch the weather for a given location, returning a short description.

    # Example Logic
    """
    return f"The weather in {location} is 22 degrees {unit}."

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
    tools = [get_weather],
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)



@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the your Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    msg = cl.Message(content="")
    await msg.send()
    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = Runner.run_streamed(
        agent, 
        input=history,
        run_config = config,
    )
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            token = event.data.delta
            await msg.stream_token(token)

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)

----------openai agent sdk---------
The Agents SDK is designed with two main goals in mind:

Enough features to be worth using, but few enough primitives to make it quick to learn.(In simple terms:
Primitives = the most basic tools, functions, or components you need to work with the SDK.
Having few primitives means you don‚Äôt have to learn many things ‚Äî just a few core ideas that can be combined or extended to build complex behavior.
üëâ Example: Instead of giving you 20 complicated functions, the SDK gives you 3 simple ones that can do everything ‚Äî making it faster and easier to learn.)
Simplicity with power ‚Äì It includes just the right amount of features to be useful without being overwhelming. You can learn it quickly because it focuses on a few key, essential
tools instead of too many complex ones.

Flexibility ‚Äì It works perfectly as soon as you install it (‚Äúout of the box‚Äù), but you also have full control to customize behaviors and logic as needed for your application.

üëâ In short:
The Agents SDK is built to be easy to learn, powerful to use, and flexible to customize.


------------------------------------------üß† Main Features Explained

Agent Loop
The SDK automatically manages the process of calling tools, sending their results to the AI model, and repeating this cycle until the task is finished.
‚úÖ In short: It handles the full ‚Äúthink ‚Üí act ‚Üí reflect‚Äù loop for you.

Python-first(python ki hi chizain istemal krty huy new chizain bnyngy jese ek decorator lgaya to graph bn gya is trh sb kch , kch bhi new ni sikhna parega)
You can use normal Python syntax and features to control and connect agents.
‚úÖ In short: No need to learn new programming styles ‚Äî it works naturally with Python.
Orchestrate = to coordinate or arrange different parts to work together smoothly (like a conductor leading an orchestra).

Handoffs
Lets one agent delegate (pass) a task to another specialized agent.
‚úÖ In short: Agents can work together and share tasks.

Guardrails
These are safety checks or validations that run alongside your agent‚Äôs logic. If something looks wrong, it can stop the process early.
‚úÖ In short: Ensures inputs and outputs are valid and safe.

Sessions
Automatically keeps conversation history so your agent remembers past context without manual coding.
‚úÖ In short: Handles memory for you.

Function Tools
Instantly turn any Python function into a usable tool.
The SDK automatically creates input/output schemas (data formats) and validates them with Pydantic (a Python library for data validation).
‚úÖ In short: Makes your Python functions ready for use by the agent with no extra effort.

Tracing
Provides visual tracking and debugging of agent workflows.
You can also use OpenAI tools for evaluation, fine-tuning, and distillation (improving models).
‚úÖ In short: Lets you see what‚Äôs happening inside and improve performance.

ü™Ñ Overall Summary
The Agents SDK gives developers powerful tools to build AI agents easily in Python.
It manages loops, memory, safety, and collaboration automatically ‚Äî while letting you monitor, debug, and extend behavior smoothly.

üëâ In one line:
The SDK helps you build, control, and monitor AI agents effortlessly using simple Python code.



----------------------------------
---generic typehints deta h



------------In Python, @dataclass is a decorator that automatically adds common methods to a class ‚Äî like __init__(), __repr__(), and __eq__() ‚Äî so you don‚Äôt have to write them manually.

üß© Meaning:
A decorator (like @dataclass) modifies how a class or function behaves ‚Äî it adds extra functionality.

üß† Why use @dataclass:
It‚Äôs used to create data-holding classes (like records or structs) more easily.

Without @dataclass üëá

class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age


With @dataclass üëá

from dataclasses import dataclass

@dataclass
class Person:
    name: str
    age: int


‚úÖ Now Python automatically creates:
__init__() ‚Äì constructor
__repr__() ‚Äì readable string output
__eq__() ‚Äì equality check

ü™Ñ In short:
@dataclass makes it faster and cleaner to define simple classes that store data ‚Äî no need to write repetitive code.


---------------------------------------DATACLASS AND PYDANTIC----------------------------
Purpose
dataclass is used to create simple data containers with less code.
Pydantic is used to validate and parse data (especially from external sources like APIs or JSON).

Type Checking
dataclass does not enforce types at runtime ‚Äî type hints are just for readability.
Pydantic checks and enforces types at runtime, converting or rejecting invalid data.

Validation
dataclass has no built-in validation. You must write custom checks.
Pydantic automatically validates inputs and raises errors if data is incorrect.

Automatic Conversion
dataclass won‚Äôt convert "25" (string) to 25 (int).
Pydantic automatically converts compatible types.

Error Handling
dataclass won‚Äôt stop you from passing wrong types.
Pydantic raises detailed validation errors.

Performance
dataclass is faster and lighter (no validation overhead).
Pydantic is slightly slower because it performs runtime validation.

Use Case
Use dataclass for internal data storage or simple models.
Use Pydantic for external data, user input, or API responses that need validation.

üëâ In short:
dataclass = simple and lightweight.
Pydantic = powerful and safe with validation.

-------------data class built-in h or pydantic ki library import install krni parti h
---dataclass m bhi methods bna skty hn  or class method or class variable bhi define krskty hn dataclass m


----------An instance variable is a variable that belongs to a specific object (instance) of a class in Python.
üß© Explanation:
Each time you create an object from a class, it gets its own copy of the instance variables.
These variables are usually defined inside the __init__() method using self.

-------------------------------------üß∞ Example:---------------------------------
class Car:
    def __init__(self, brand, color):
        self.brand = brand      # instance variable
        self.color = color      # instance variable

# creating two objects
car1 = Car("Toyota", "Red")
car2 = Car("Honda", "Blue")

print(car1.brand)  # Toyota
print(car2.brand)  # Honda


Here:
brand and color are instance variables.
car1 and car2 each have their own separate values for these variables.

ü™Ñ In short:
An instance variable stores data unique to each object created from a class.


---------class method ko class ky sth call krskty hn
--------instance method keliye class ka phly ham instance bnaty hn or instance bnany ky bd phr ham usy use krhy hoty hn





---------------------------instance vs class
ek h class level or ek h object(instance) level
ab ham masjid jaty hn whn namaz shuru hony ka time sbke liye same h to masjid jo h wo class(class level per) h or us m hr bnda jo ha(object , instance) uske pas apni apni watch h us per
kisi ka time ek mnt agay h kisi ka fix h to yh ha object level


------------------------------------------------------------------
üß† 1. Instance Variable
Belongs to a specific object (instance) of a class.
Defined using self inside __init__().
Each object has its own copy of these variables.

‚úÖ Example:

class Car:
    def __init__(self, brand):
        self.brand = brand  # instance variable

üè∑Ô∏è 2. Class Variable
Shared by all objects of the class.
Defined outside any method, usually directly under the class.
Changing it affects all instances (unless overridden).

‚úÖ Example:

class Car:
    wheels = 4  # class variable
    def __init__(self, brand):
        self.brand = brand  # instance variable

‚öôÔ∏è 3. Instance Method
Works with instance variables.
Needs access to the object (self).
Used to define behavior for individual objects.

‚úÖ Example:

class Car:
    def start(self):  # instance method
        print("Car started")

üß© 4. Class Method
Works with class variables, not instance variables.
Defined using the @classmethod decorator.
Takes cls (class reference) instead of self.

‚úÖ Example:

class Car:
    wheels = 4

    @classmethod
    def change_wheels(cls, new_count):
        cls.wheels = new_count

----------------------------------------
üîπ Instance Variable
Belongs to one specific object.
Defined inside __init__() using self.
Each object has its own copy.

üîπ Class Variable
Shared by all objects of the class.
Defined outside any method (inside the class).
Changing it affects all instances.

üîπ Instance Method
Works with instance variables.
Takes self as the first parameter.
Defines behavior for individual objects.

üîπ Class Method
Works with class variables.
Takes cls as the first parameter.
Defined using @classmethod.
Affects the entire class, not just one object.


üëâ Simple Summary:
Instance variable/method ‚Üí specific to each object.
Class variable/method ‚Üí shared by all objects.



----------------------------dataclass code run both in cursor or colab-------------------------------

from dataclasses import dataclass
from typing import TypeVar,ClassVar

@dataclass         #decorator h
class American:                      #same as "class American(object):" yh (object) dalny ki zrort ni hoti kunke compiler khud dal deta h actually interpreter
  name: str
  age: int
  weight: float
  liked_food: str
  national_language: ClassVar[str] = "english"      #[str] is called generics
  national_food : ClassVar[str] = "Hamburger"           
  normal_body_temperatur: ClassVar[float] = 98.6

  def speaks(self):         #instance funcction bn gya
    return f"{self.name} is speaking {American.national_language}"

  def eats(self):
    return f"{self.name} is eating"

  @staticmethod               #( yh class ka static method h to yh srf class ky zariye hi call hoskta h,yh instance(object) se belong ni krta)
  def country_language():
    return American.national_language

    
john = American(name="isha",age=27,weight=67.5)          #(hr object ka apna naam apna age weight or language hoga or wo do method hongy speaks or eats jo unka individual hoga yh
ek object tha john asy or bhi bnaskty bob,alice or bhi bht sary unke pas bhi 4 variable or 2 methods individual hongy)
print(john.speaks())
print(john.eats())
print(American.country_language())

print(john.name)
print(john.age)
print(john.weight)
print(john)
print(American.national_language)



---------------class ka object bnarhy hn to usy instancetiate krrhy hhoty hn usy callable ni bolengy


---------------------------------public , protected and private variable-------------------------------------
üîπ Protected Variables
Named with a single underscore _variable.
Means: ‚Äúfor internal use only‚Äù (not enforced, just a convention).
Can still be accessed from outside the class if needed.

‚úÖ Example:

from dataclasses import dataclass

@dataclass
class Car:
    _speed: int  # protected variable

car = Car(100)
print(car._speed)  # allowed, but not recommended


üîπ Private Variables
Named with double underscores __variable.
Python automatically name-mangles it (changes the name internally).
Not directly accessible from outside the class.

‚úÖ Example:

from dataclasses import dataclass

@dataclass
class Car:
    __engine_number: str  # private variable

car = Car("EN123")
# print(car.__engine_number)  ‚ùå Error
print(car._Car__engine_number)  # ‚úÖ Works (name mangling)


üîπ Public Variables in Dataclass
Variables without underscores are public.
They can be freely accessed and modified from anywhere.
This is the default in Python (and in dataclasses).


‚úÖ Example:
from dataclasses import dataclass

@dataclass
class Car:
    brand: str      # public
    model: str      # public

car = Car("Toyota", "Corolla")
print(car.brand)   # ‚úÖ Accessible
car.model = "Yaris"  # ‚úÖ Can modify


ü™Ñ In short:
brand ‚Üí Public (default, fully accessible)
_speed ‚Üí Protected (for internal use)
__engine_number ‚Üí Private (hidden from outside)


ü™Ñ In short:
_var ‚Üí Protected (accessible but meant for internal use).
__var ‚Üí Private (hidden via name mangling).
Works the same in dataclasses as in normal Python classes ‚Äî the decorator doesn‚Äôt change this behavior.



---------------static variables static method m bhi available hoty hn or instance methods m bhi available hoty h
---------class variable available hoty hn sary objects m or unka sary objects m ek hi value hoti h


---------system prompt wo instruction jo ham llm ko pass krrhy hn ky yh tumhara persona h yh goal h yh type h
----instruction ya to string hogi ya callable hogi ya none hogi or default m equals to none hogi
1.string(str) hamne llm ko pas kia or llm ne isy instruction ky tor per lia or llm ne response krdia phr
2.dusra instruction callable bhi hoskta h mtlb ham dynamically koi bhi chez agent ya llm ko pass krskta hu instructions ky tor per(koi bhi function hoskta ha jo kch kam krny ky bd
dynamically system prompt hamy return krwata hoga string ky andr or wo pass hojyga llm ko)
3.none mtlb koi bhi instruction pas ni krrhy none pas krrhy



---------------------------str,callable,none-----------------------------------
This means that the instruction (a variable or parameter) can accept three possible types of values:

üîπ 1. str (String)
It can be a text instruction ‚Äî a normal string.
‚úÖ Example:
instruction = "Start the process"

üîπ 2. callable
It can be a function or method that can be called/executed.
‚úÖ Example:

def greet():
    print("Hello!")

instruction = greet  # callable
instruction()        # runs the function


Meaning: You can pass a function itself instead of text, so it can be executed later.

üîπ 3. None
It means no instruction is given (empty or inactive).
‚úÖ Example:
instruction = None

ü™Ñ In short:
instruction can be:
A string ‚Üí simple message or text command.
A callable ‚Üí a function that can run.
None ‚Üí no instruction at all.



---------------------------CALLABLE---------------------------
--------------------üß∏ Example 1 ‚Äî Function

def say_hello():
    print("Hello!")

say_hello()   # You called it! It says Hello!


üí° The function is callable because you can call it with ().




----------------------üöó Example 2 ‚Äî Class with a method
class Car:
    def start(self):
        print("Vroom!")

my_car = Car()
my_car.start()  # You called the method ‚Äî the car starts!


üí° my_car.start is callable because you can use () to make it do something.



---------------üß© Example 3 ‚Äî A callable object

You can even make an object callable by giving it a special power called __call__.

class Greeter:
    def __call__(self):
        print("Hi there!")

g = Greeter()
g()  # works like a function!


üí° g is not a function ‚Äî it‚Äôs an object ‚Äî but you can still ‚Äúcall‚Äù it. That‚Äôs why it‚Äôs callable!



-------------------------‚ùå Example 4 ‚Äî Not callable
toy = "Teddy bear"
toy()  # ‚ùå Error! Strings can‚Äôt be called


üí° You can‚Äôt ‚Äúcall‚Äù a string ‚Äî it‚Äôs not callable.



--------------------------------------------ü™Ñ Check if something is callable
print(callable(say_hello))  # True
print(callable(toy))        # False

------------üåü In short:
Callable = something you can call with ()
Examples: functions, methods, or special objects.
If you can‚Äôt ‚Äúcall‚Äù it, Python gives an error.


----------üßÅ Think of it like this:
A callable is like a button you can press (with ()) to make something happen!


-------------------------
def greet():
  return "hello"

print(greet())
dir(greet())      #dir jo ha wo greet() function ki puri directory dikha dega
#dir(greet) ‚Üí lists everything Python stores inside the function object, like its built-in properties.

output:
hello
['__add__',
 '__class__',
 '__contains__',
 '__delattr__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getitem__',
 '__getnewargs__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__iter__',
 '__le__',
 '__len__',
 '__lt__',
 '__mod__',
 '__mul__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__rmod__',
 '__rmul__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 'capitalize',
 'casefold',
 'center',
 'count',
 'encode',
 'endswith',
 'expandtabs',
 'find',
 'format',
 'format_map',
 'index',
 'isalnum',
 'isalpha',
 'isascii',
 'isdecimal',
 'isdigit',
 'isidentifier',
 'islower',
 'isnumeric',
 'isprintable',
 'isspace',
 'istitle',
 'isupper',
 'join',
 'ljust',
 'lower',
 'lstrip',
 'maketrans',
 'partition',
 'removeprefix',
 'removesuffix',
 'replace',
 'rfind',
 'rindex',
 'rjust',
 'rpartition',
 'rsplit',
 'rstrip',
 'split',
 'splitlines',
 'startswith',
 'strip',
 'swapcase',
 'title',
 'translate',
 'upper',
 'zfill']



-----------------------------------callable vs non-callable object------------------------
--------NON-CALLABLE OBJECT CODE----------------
from dataclasses import dataclass

@dataclass
class Human:
    # Attributes
    name: str
    age: int

    # Method to greet
    def greet(self):
        return f"HI, I'm {self.name}"

    # Another method
    def works(self):
        return "I am working"

# Object instantiation (partially visible in the screenshot)
obj1 = Human(name="John", age=22)

print(obj1.name)         #OUTPUT: John
print(obj1.age)          #OUTPUT: 22
print(obj1.greet())      #OUTPUT:HI,I'm John
print(obj1.works())      #OUTPUT:I am working
obj1()                   #IT GIVES ERROR:---------------------------------------------------------------------------
                        TypeError                                 Traceback (most recent call last)
                        /tmp/ipython-input-1288370025.py in <cell line: 0>()
                                ----> 1 obj1()
                            TypeError: 'Human' object is not callable




---------------------------CALLABLE OBJECT CODE----------------------------------
from dataclasses import dataclass

@dataclass
class Human:
    # Attributes
    name: str
    age: int

    # Method to greet
    def greet(self):
        return f"HI, I'm {self.name}"

    # Another method
    def works(self):
        return "I am working"

    def __call__(self):          #yh magic method bhi kehty hn or dunder function bhi dunder means double underscore to isy jo hamara object h wo callable hojyga or output ajana
      return "hello"
      
# Object instantiation (partially visible in the screenshot)
obj1 = Human(name="John", age=22)

print(obj1.name)         #OUTPUT: John
print(obj1.age)          #OUTPUT: 22
print(obj1.greet())      #OUTPUT:HI,I'm John
print(obj1.works())      #OUTPUT:I am working
obj1()                   OUTPUT: 'hello'



----------------bydefault sary function dunder hoty hn callable hoty hn lekin jo instance hota h means object to wo callable ni hota usko callable bnana hamne upr dekha
-----class m () ky andr self ni lgaty ya zrori ni baqi sb function m lgaty self () is m or self jo h wo class ka refrence h ky class ky andr ek method h to usy call krny keliye self
dengy mtlb yh isi class ka h self h yh khud h 





-------------------------------------------SELF UNDERSTANDING----------------------
-----------------;;;;;;;;;;;;;;;;;;;QUESTION;;;;;;;;;;;;;;;;;;;;;;;;;---------------------
@dataclass
class Human:
    # Attributes
    name: str
    age: int

    # Method to greet
    def greet(self):
        return f"HI, I'm {self.name}"

    # Another method
    def works(self):
        return "I am working"
----( in this code in greet method we are passing self and in class Human we are not passing self why? and what does self means? what is refer?


---------------------------------;;;;;;;;;;;;;;;;ANSWER;;;;;;;;;;;------------------
üß† Step 1: Understanding what a class really is

A class is just a blueprint ‚Äî it tells Python what data (attributes) and what actions (methods) an object will have.

Example:

@dataclass
class Human:
    name: str
    age: int


Here, we are only defining the structure.
We‚Äôre saying: ‚ÄúEvery Human will have a name and an age.‚Äù
So we don‚Äôt need to pass self here ‚Äî because this part just describes what a Human looks like, not what it does.

üß© Step 2: When we define a method inside the class
Now look at:

def greet(self):
    return f"Hi, I'm {self.name}"

Here we are defining an action ‚Äî something a Human object can do.
So, when you create an object:

person = Human("Queen", 20)

and then call:

person.greet()


Python secretly calls it like this:

Human.greet(person)


That means the object person (which is Human("Queen", 20)) is automatically passed to the function as the first argument ‚Äî and that‚Äôs what self refers to.

üí° Step 3: What self means

üëâ self simply means ‚Äúthis object‚Äù.
It refers to the specific object that is calling the method.

So when you write:

return f"Hi, I'm {self.name}"


self.name means ‚Äî ‚Äúget the name value that belongs to this particular Human object.‚Äù

ü™Ñ Step 4: Why not in the class header?
Because the class header defines the blueprint (the design), not an instance (object).
self only exists after you create an object from the class.

-----------------üí° Summary:
self means ‚Äúthis object‚Äù ‚Äî it refers to the object that calls the method.
You write self as the first parameter in methods inside a class.
When you call a method like person.greet(), Python secretly does Human.greet(person).
That‚Äôs why the object (person) is automatically passed as self.
self lets you access object attributes (like self.name or self.age).
In the class header, you don‚Äôt write self because you‚Äôre defining a blueprint, not a specific object.
self is only used inside methods that work with individual objects.



-------------QUESTION:what is diffrence between def __init__ or def __call__ and why we dont make def __init__ in dataclass and we make def __init__ it in normal class?----------
--------------------------------------------------ANSWER--------------------------------------
üí° 1. __init__ vs __call__
üß© __init__:

It‚Äôs the initializer (constructor) method of a class.
It runs automatically when you create an object.
Its job is to set up the object‚Äôs attributes.

Example:

class Human:
    def __init__(self, name, age):
        self.name = name
        self.age = age


Now:
person = Human("Queen", 20)

Here, Python automatically calls __init__(person, "Queen", 20).


ü™Ñ __call__:
It makes an object itself callable like a function.
If a class defines __call__, you can do object() as if it were a function.

Example:

class Greet:
    def __call__(self):
        return "Hello!"

g = Greet()
print(g())  # Notice the parentheses! ‚Üí Calls __call__

So __call__ lets an object behave like a function.

üí´ 2. Why we don‚Äôt write __init__ in a dataclass
When you use @dataclass, Python automatically creates the __init__ method for you!

Example:

from dataclasses import dataclass

@dataclass
class Human:
    name: str
    age: int


is the same as writing:

class Human:
    def __init__(self, name, age):
        self.name = name
        self.age = age


So, dataclasses save you from boilerplate code ‚Äî you don‚Äôt need to manually write __init__, __repr__, or __eq__ unless you want to customize them.

üß† Summary in Points
__init__ ‚Üí sets up object attributes when object is created.
__call__ ‚Üí allows an object to be called like a function.
Dataclasses automatically generate __init__ for you.
Normal classes need you to manually define __init__.
You can still define your own __init__ in a dataclass if you need custom setup.





--------------------QUESTION:WHEN me make dataclass so and make class then we make object of it so whats basically the scenario that connect it all together how object is making
here with dataclass-------------------------------------
---------------------------------------------------------------------ANSWER-----------------------------------------
üå∏ 1. Normal class ‚Äî what happens when you make an object
When you create a normal class, you must define your own __init__ (constructor):

class Human:
    def __init__(self, name, age):
        self.name = name
        self.age = age


Now when you write:

person = Human("Queen", 20)


Python does this internally:
Creates empty space in memory for person (a Human object).
Calls __init__(person, "Queen", 20).
Fills in the data ‚Üí person.name = "Queen" and person.age = 20.

üí´ 2. Dataclass ‚Äî what changes
When you use @dataclass, Python automatically creates that __init__ method for you.

from dataclasses import dataclass

@dataclass
class Human:
    name: str
    age: int


You don‚Äôt see it, but Python secretly generates this behind the scenes:

def __init__(self, name: str, age: int):
    self.name = name
    self.age = age


So when you write:

person = Human("Queen", 20)


The same exact process happens:
Memory is allocated for person.
The auto-generated __init__ runs.
It stores name and age inside that object.

üß© 3. So what‚Äôs connecting it all together?
Here‚Äôs the chain of connection in simple steps:
@dataclass tells Python ‚Üí ‚ÄúPlease create extra magic methods for this class.‚Äù
One of those methods is __init__.
When you call Human("Queen", 20), Python automatically calls that generated __init__.
That __init__ puts your values into the object (self.name, self.age).
You now have a fully formed object (person) with data inside it.

üí° 4. In short:
@dataclass = a decorator that auto-creates code (like __init__, __repr__, __eq__).
When you create an object, the auto-generated __init__ runs behind the scenes.
So the object creation flow stays exactly the same, only with less code to write.



---------------------------------QUESTION : __new__ vs __init__ ------------------------------------------------------
---------------------------------------ANSWER------------------------------------------
üí° Difference between __new__ and __init__ in Python
üß© 1. __new__ ‚Üí creates the object
ü™Ñ 2. __init__ ‚Üí initializes the object

They both work together, but do different jobs.

üß± Step-by-step example
When you write:

person = Human("Queen", 20)

Python does this under the hood:

Calls Human.__new__(Human) ‚Üí this actually creates the empty object in memory.

Then automatically calls Human.__init__(person, "Queen", 20) ‚Üí this fills in the data (sets attributes).

So:
__new__ = ‚Äúmake me an empty box‚Äù
__init__ = ‚Äúfill that box with stuff‚Äù

üß† Example
class Human:
    def __new__(cls):
        print("Step 1: Creating object (memory allocated)")
        instance = super().__new__(cls)
        return instance

    def __init__(self):
        print("Step 2: Initializing object (attributes set)")


Output:

Step 1: Creating object (memory allocated)
Step 2: Initializing object (attributes set)

ü™Ñ Simple Analogy
Imagine you‚Äôre baking a cake üç∞
__new__ ‚Üí builds the empty cake base (the sponge).
__init__ ‚Üí decorates it (adds cream, frosting, etc).
You can‚Äôt decorate before you bake ‚Äî same logic!

üí¨ Why we rarely use __new__
You almost never need to define __new__ in normal classes or dataclasses.
It‚Äôs mainly used when creating immutable objects (like tuples, strings) or customizing metaclass behavior.
In regular classes, we only define __init__.

üå∏ Summary in Points
__new__ ‚Üí creates the object (allocates memory).
__init__ ‚Üí initializes the object (sets its attributes).
Python always calls __new__ first, then __init__.
You usually define only __init__; __new__ is for advanced use.
In dataclasses, __init__ is auto-generated; __new__ is still used internally by Python.



-----------run_sync method static h runner ki class m 




---------------------RSI----------------------
In AI, RSI usually stands for Recursive Self-Improvement.

Here‚Äôs a quick breakdown üëá
ü§ñ RSI (Recursive Self-Improvement) in AI

Meaning:
It‚Äôs when an artificial intelligence system improves its own design or capabilities ‚Äî and each improved version becomes better at improving itself.
In simple words:

The AI keeps making itself smarter, faster, and more efficient ‚Äî on its own.

‚öôÔ∏è How it works (conceptually)
AI learns to modify its own algorithms or parameters.
Creates a better version of itself.
The new version repeats the process ‚Äî improving even faster.
This can lead to an ‚Äúintelligence explosion‚Äù, where AI rapidly surpasses human intelligence.

üåç Example idea
An AI starts by optimizing its neural network structure.
It learns better data-processing methods.
It rewrites parts of its own code for efficiency.
Each cycle = smarter AI than before.

‚ö†Ô∏è Risks & Discussion
Uncontrolled self-improvement might make AI unpredictable.
It‚Äôs a major topic in AI safety and ethics research.
The concept ties closely to Artificial General Intelligence (AGI).





































