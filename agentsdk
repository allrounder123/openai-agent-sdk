
-----------agent jo h wo autonomous action lene wala ek model h--------------------------
------async mtlb ham request bhjty hn wo process krta h or jb uska process khtm hoga wo response dedega-------------------------
--------openai ky sdk ko use krty huy ham apny customize llm ko use krty hn jese openai m gemini flash use krty-----------------sdk agent or sary ky sary features sdk ky hongy lekin 
backend per jo llm hoga wo free wala hoga gemini ka-------------demagh jo h wo gemini ka or jo decision making wagaira h wo openai sdk ka h
-----------sync mtlb task jb perform horha h ussi wqt response do , or async mtlb 10 15 mnt bhi sochlo or jb response complete hojye tb response dedo
--------Runner m bht sary function method hoty by default async hota or ham Runner.run_sync krty hn (run_sync method h Runner m)(client to server communication jo krwaty hn wo sary 
tareeky runner m mojud hn)
------sync mtlb mene apko kaha biryani lekr ao to jb tk ap biryani ni laogy tb tk m wait krungi kisi  se bt ni krungi intezar krungi or async mtlb mene kaha ap biryani lekr ao or jb tk ap
ni aty tb tk m baqi kaam nipta lungi bt krlungi sbse kaam krlungi




-----------------------
In the OpenAI Agents SDK (and similar frameworks), youâ€™ll often see three levels of logic:
Global level â†’ Agent level â†’ Run level

Letâ€™s break each down simply ğŸ‘‡
ğŸŒ Global Level
The topmost layer â€” things that apply to the whole app or system, not just one agent or one conversation.
It defines shared settings, global tools, and event handlers that affect everything.

Think of it as:
ğŸ§  â€œWhat rules or tools should all agents or runs in my app have access to?â€

Examples:
Registering a tool that every agent can use (like a database connector).
Defining logging or error-handling behavior for the whole app.
Setting up startup events, environment variables, or authentication.

ğŸ§© Agent Level
Defines the behavior and configuration of one specific agent.
Example: system prompt, tool access, goals, or memory setup.
Agents can inherit or override global settings.

Think of it as:
ğŸ¤– â€œWho the agent is and what it can do.â€

âš™ï¸ Run Level
The execution layer â€” what happens in a single conversation or task run.
This is where the agent actually processes input, calls tools, and produces outputs.

Think of it as:
ğŸƒ â€œWhat happens right now while the agent is running.â€





---------------------SYNC FUNCTION IN RUNNER--------------(WORK ON GOOGLE COLAB)--------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

math_agent: Agent = Agent(name="MathAgent",
                     instructions="You are a helpful math assistant.",
                     model=llm_model) # gemini-2.5 as agent brain - chat completions

result: Runner = Runner.run_sync(math_agent, "why learn math for AI Agents?", run_config=config)

print("\nCALLING AGENT\n")
print(result.final_output)




-------------------------------------------------------------------------------------------------------------------------------------
-------------ASYNC FUNCTION IN RUNNER----------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio(is library se koi bhi async function chla skty hn)

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)---(run time configuration)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = await Runner.run(agent, "Tell me about recursion in programming.", run_config=config)-----------(.run ka method Runner m by-default async hi hota h)( or await mtlb jo kam 
krrha h uska wait krega or jb wo response ajyga usko result m dal dega)
    print(result.final_output)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.

if __name__ = "__main__":(is line ki zrort .py file m hoti h jo vs or cursor m hota h yhn iski zrort ni h kunke google colab m import asyncio krdia h hamne jo async ky sary function chlata ha
asyncio.run(main())



--------------chatgpt foran reply krta h to wo async h agr kisi kam per soch kr phr response kry to wo sync

-------------------STREAMED FUNCTION IN RUNNER-----------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = Runner.run_streamed(agent, "Tell me about recursion in programming.", run_config=config)
    print(result)
    async for e in result.stream_events():
      print(e)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.


asyncio.run(main())


-----------------------------------streamed code run in google colab---------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
import os
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
from agents.run import RunConfig
from google.colab import userdata
import asyncio

from agents import (
    Agent,
    Runner,
    set_default_openai_api,
    set_default_openai_client,
    set_tracing_disabled,
)

gemini_api_key = userdata.get("GEMINI_API_KEY")


# Check if the API key is present; if not, raise an error
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.")

#Reference: https://ai.google.dev/gemini-api/docs/openai
external_client = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

model = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

set_default_openai_client(client=external_client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner


async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
        model=model
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):-----------------(ResponseTextDeltaEvent: event data ky class ki type bhi hogi yh type h 
data ki jo wo dega,raw_response_event:loop lgaya h jo us betahasha chizain arhi hn to hamne kaha raw response event jb hoga us wqt wo kam complete krke data return krta h)
            print(event.data.delta, end="", flush=True)-----------(event.data.delta: yh text nikalyga jitna complete hua h
 ( end=""----: by default print jb horha hota h to wo \n se next line per ajata h to hamne kehdia ky next line ni chaiye jo arha h usy dusry ky agay likhty jao space dal kr

asyncio.run(main())



------In Python, flush=True in the print() function forces the output to be written to the console immediately. Normally Python buffers output, which means it might delay displaying 
what you printed until the buffer is full or a newline character is encountered. Using flush=True is particularly useful in scenarios like real-time logging or when printing streaming
data, ensuring that every piece of output is displayed as soon as it's generated.

For example, in your code:
print(event.data.delta.end="", flush=True)

end="": This means no newline character is appended at the end of the printed text.
flush=True: This forces the output to be flushed to the console immediately, even without a newline

------------------------FLUSH=TURE IN OUR CODE JO UPR GYA H------------------------
ğŸ§  Meaning of flush=True
Normally, Python buffers printed text â€” meaning it stores it temporarily before actually showing it on the screen.
So when you print inside a loop, the text might not appear immediately.
âœ… flush=True forces Python to immediately push (flush) the text from the buffer to the console or output stream.

ğŸ’¬ In this context (streaming output)
Here, the model sends chunks of text as it generates them (event.data.delta = partial text).
To make the output appear live, like real-time typing, we use:
flush=True

That way, each small piece (token or word) appears instantly instead of waiting until the model finishes.

âš™ï¸ Example to visualize it
Without flush:
# Text appears all at once at the end
print("H", end="")
print("e", end="")
print("l", end="")
print("l", end="")
print("o", end="")

â†’ Output might show â€œHelloâ€ only after the loop ends.
With flush:
# Text appears letter by letter
print("H", end="", flush=True)
print("e", end="", flush=True)
print("l", end="", flush=True)
print("l", end="", flush=True)
print("o", end="", flush=True)

â†’ Output shows H e l l o in real-time.

ğŸ§¾ Summary
flush=True â†’ forces the output to appear immediately.
Used in streaming to show model text live as itâ€™s generated.
Without it, the console might delay or group text output.

âœ… In short:
flush=True makes the text appear instantly during streaming â€” creating that smooth, â€œtyping liveâ€ effect.




----------------------stream ky ilawa event bhi perform horhy hoty h kunke agent ko hamne task dia to wo bht sary kaam krrha hota ha tool ko cl bhi krta h uska output bhi lata h to bht
sary event perform horhy hoty hn hamne upr waly code m to bs ek event nikala tha jis m wo text complete krke return krta ha to kon kon se event perform hoty hn agr unhy dekhna chahain to?
same steps wohi hn bs thori changes hn jo yh nichy waly code m hui h-----------------------------------------------------------

import asyncio
import random

from agents import Agent, ItemHelpers, Runner, function_tool


@function_tool--------(yh tool decorator function use kia h function ko tool bnadeta h yh)
def how_many_jokes() -> int:------------(yh normal function h jo random number generate krta ha)
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],-------------(jo tool bnaya tha upr wo hamne yhn apne agent ko provide krdia)
        model=model
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",

    )
    print("=== Run starting ===")
    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types





asyncio.run(main())

print("=== Run complete ===")


---------------------chainlit+stream runner in currsor--------------------------------------
import chainlit as cl
from agents import Agent, Runner,RunConfig, AsyncOpenAI, set_default_openai_client, set_tracing_disabled, set_default_openai_api,OpenAIChatCompletionsModel
from agents import function_tool
import os
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from dotenv import load_dotenv , find_dotenv
load_dotenv()
from agents import enable_verbose_stdout_logging
enable_verbose_stdout_logging()

gemini_api_key = os.getenv('GEMINI_API_KEY')

set_tracing_disabled(True)
set_default_openai_api("chat_completions")

provider = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

set_default_openai_client(provider)

model= OpenAIChatCompletionsModel(
    model="gemini-2.0-flash",
    openai_client=provider
)

config = RunConfig(
    model = model,
    model_provider = provider,
    tracing_disabled= True
)

agent: Agent = Agent(
    name="Panaversity manager",
    instructions="You are a helpful assistant",
    model="gemini-2.0-flash",
  )

result = Runner.run_sync(agent, "what is 22 * 13 + 32 - 8 ", run_config = config)

print(result.final_output)





@cl.on_chat_start
async def handle_chat_start():
    cl.user_session.set("history", [])

    await cl.Message(content="Hello! I'm the Panadversity Support Agent.").send()

@cl.on_message
async def main(message: cl.Message):
    history = cl.user_session.get("history")

    msg = cl.Message(content="")
    await msg.send()
    # Standard Interface ({"role": "user", "content": "Hello!"}, {"role": "
    history.append({"role": "user", "content": message.content})

    result = Runner.run_streamed(
        agent, 
        input=history,
        run_config = config,
    )
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            token = event.data.delta
            await msg.stream_token(token)

    history.append({"role": "assistant", "content": result.final_output})
    cl.user_session.set("history", history)
   
 

--------------------------------------------------------------------------------------------
-------------------IMPORTANT:
Thatâ€™s one of the 3 main ways to run an agent in the Agents SDK:

run_sync â†’ normal, blocking
run_async â†’ async, non-blocking
run_streamed â†’ streaming, real-time output


âš¡ What is â€œstreamingâ€?
Streaming means:
The model starts sending partial output while itâ€™s still generating the rest.
So instead of waiting for the whole response (like sync/async do), your code receives chunks (events) of the output as soon as theyâ€™re available.

Think of it like:
Non-streamed (sync/async): Wait till the whole essay is done â†’ print once.
Streamed: Show words as theyâ€™re typed â†’ print gradually, real-time.

ğŸ§  Runner.run_streamed() â€” what it does
It starts an asynchronous run.
Immediately returns a streaming result object (not the final text).
That result object emits events as the model generates output.

---------------You can then do:CODE-----------------

async for e in result.stream_events():
    print(e)

â†’ This listens for events like tokens, messages, or tool calls as they happen.
This is perfect for live chat UIs (like Chainlit, Gradio, or Colab outputs).

------------------------------------------------------------------------------

ğŸ”„ Runner.run_async() â€” what it does
Runs the same logic but waits until the full response is ready before returning.
You can await it to get the entire message result directly.
No live token-by-token updates.

Example:

result = await Runner.run_async(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you just need the full answer, not a live stream.
--------------------------------------------------------------------------------------

â³ Runner.run_sync() â€” what it does
Same as run_async but for non-async (normal) Python code.
Blocks until the full run completes (like a normal function).

Example:

result = Runner.run_sync(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: youâ€™re in a plain Python script (no asyncio).




---------------------------------------------------------------------
ğŸ’¬ In your code:

result = Runner.run_streamed(agent, "Tell me about recursion...", run_config=config)

Starts the agent.
Doesnâ€™t wait for full completion.
Gives you a stream handle in result.

Then this part:

async for e in result.stream_events():
    print(e)

prints the tokens or partial responses live, as they arrive.
Thatâ€™s how you get â€œlive typingâ€ or real-time generation.


---------------------------------------------
ğŸ§  What is Runner?
Runner is a controller class in the OpenAI Agents SDK thatâ€™s responsible for executing (running) an agent.
Think of it as the engine that takes:
your Agent (the brain),
a user input (the question),
a RunConfig (the setup),
and actually runs the full conversation or reasoning process.

âš™ï¸ In short:
Runner = the component that manages how an agentâ€™s logic is executed â€” including tool calls, message handling, and LLM requests.
---runner jo h wo client to server communication krwata ha or communication sync hogi asyn hogi ya streamed hogi yh define krty.


---------------------------------------
ğŸ§± Analogy â€” AI system as a car
Agent â†’ The driver â€” knows what to do and how to respond.
RunConfig â†’ The GPS/settings â€” defines which model, API key, and configuration to use.
Runner â†’ The engine â€” actually runs the process and makes the agent work.
Without the Runner, the agent just exists but canâ€™t perform any actions.

ğŸ§° Runnerâ€™s Main Methods
run_sync() â†’ Runs the agent normally (blocking execution until done).
run_async() â†’ Runs the agent asynchronously using await (non-blocking).
run_streamed() â†’ Runs the agent in streaming mode, giving partial outputs live while generating.







---------------------TOOL CALLING------------------------------
In AI, a tool is an external helper function or service that an AI model can use to perform specific tasks it canâ€™t do on its own â€” like checking the weather, running code, searching
the web, or doing calculations.
The process of tool calling means the AI automatically decides when and how to use these tools during a conversation.
Itâ€™s like the AI saying:
â€œI canâ€™t do this myself, but I know a tool that can!â€
Then it calls the tool, gets the result, and uses it to respond naturally.

ğŸ§­ Why Tools Are Important
ğŸ•’ Fetch live data â€” e.g., current weather, stock prices, or news.
âš™ï¸ Perform actions â€” e.g., send an email, run Python code, access a database.
âœ… Improve accuracy and safety â€” by verifying answers using reliable sources.

ğŸ“ Summary (in points)
A tool is a helper function that performs one specific task.
Tool calling is when the AI uses that helper automatically during chat.
Tools extend what AI can do (fetch real data, act, or validate info).
The AI uses tools just like a person uses a calculator â€” decides when to use it, gets a result, and explains it.
Tools make AI more useful, interactive, and reliable.


----------------------------------------simple tools code------------------------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")


# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

#making tool here
from agents.tool import function_tool

@function_tool("get_weather")
def get_weather(location: str, unit: str = "C") -> str:
    """
    fetch the weather for a given location, returning a short description.

    # Example Logic
    """
    return f"The weather in {location} is 22 degrees {unit}."


@function_tool("puaic_student_finder")
def student_finder(student_roll: int) -> str:
    """
    find the PUAIC student based on the roll number """

    Data = {1: "Basim", 2: "Sir Zia", 3: "Amaan"}
    return data.get(student_roll,"not found")


import asyncio
from agents import Agent,Runner

async def main():
  agent = Agent(
      name = "assistant",
      instructions="you are help assistant",
      tools = [get_weather,student_finder],
      model = llm_model
  )

  result = await Runner.run(agent,"call get_weather tool and tell me weather of lahore")
  print(result.final_output)


if __name__ == "__main__":
    asyncio.run(main())



----------opena agent sdk---------
The Agents SDK is designed with two main goals in mind:

Enough features to be worth using, but few enough primitives to make it quick to learn.(In simple terms:
Primitives = the most basic tools, functions, or components you need to work with the SDK.
Having few primitives means you donâ€™t have to learn many things â€” just a few core ideas that can be combined or extended to build complex behavior.
ğŸ‘‰ Example: Instead of giving you 20 complicated functions, the SDK gives you 3 simple ones that can do everything â€” making it faster and easier to learn.)
Simplicity with power â€“ It includes just the right amount of features to be useful without being overwhelming. You can learn it quickly because it focuses on a few key, essential
tools instead of too many complex ones.

Flexibility â€“ It works perfectly as soon as you install it (â€œout of the boxâ€), but you also have full control to customize behaviors and logic as needed for your application.

ğŸ‘‰ In short:
The Agents SDK is built to be easy to learn, powerful to use, and flexible to customize.


------------------------------------------ğŸ§  Main Features Explained

Agent Loop
The SDK automatically manages the process of calling tools, sending their results to the AI model, and repeating this cycle until the task is finished.
âœ… In short: It handles the full â€œthink â†’ act â†’ reflectâ€ loop for you.

Python-first(python ki hi chizain istemal krty huy new chizain bnyngy jese ek decorator lgaya to graph bn gya is trh sb kch , kch bhi new ni sikhna parega)
You can use normal Python syntax and features to control and connect agents.
âœ… In short: No need to learn new programming styles â€” it works naturally with Python.
Orchestrate = to coordinate or arrange different parts to work together smoothly (like a conductor leading an orchestra).

Handoffs
Lets one agent delegate (pass) a task to another specialized agent.
âœ… In short: Agents can work together and share tasks.

Guardrails
These are safety checks or validations that run alongside your agentâ€™s logic. If something looks wrong, it can stop the process early.
âœ… In short: Ensures inputs and outputs are valid and safe.

Sessions
Automatically keeps conversation history so your agent remembers past context without manual coding.
âœ… In short: Handles memory for you.

Function Tools
Instantly turn any Python function into a usable tool.
The SDK automatically creates input/output schemas (data formats) and validates them with Pydantic (a Python library for data validation).
âœ… In short: Makes your Python functions ready for use by the agent with no extra effort.

Tracing
Provides visual tracking and debugging of agent workflows.
You can also use OpenAI tools for evaluation, fine-tuning, and distillation (improving models).
âœ… In short: Lets you see whatâ€™s happening inside and improve performance.

ğŸª„ Overall Summary
The Agents SDK gives developers powerful tools to build AI agents easily in Python.
It manages loops, memory, safety, and collaboration automatically â€” while letting you monitor, debug, and extend behavior smoothly.

ğŸ‘‰ In one line:
The SDK helps you build, control, and monitor AI agents effortlessly using simple Python code.



----------------------------------
---generic typehints deta h



------------In Python, @dataclass is a decorator that automatically adds common methods to a class â€” like __init__(), __repr__(), and __eq__() â€” so you donâ€™t have to write them manually.

ğŸ§© Meaning:
A decorator (like @dataclass) modifies how a class or function behaves â€” it adds extra functionality.

ğŸ§  Why use @dataclass:
Itâ€™s used to create data-holding classes (like records or structs) more easily.

Without @dataclass ğŸ‘‡

class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age


With @dataclass ğŸ‘‡

from dataclasses import dataclass

@dataclass
class Person:
    name: str
    age: int


âœ… Now Python automatically creates:
__init__() â€“ constructor
__repr__() â€“ readable string output
__eq__() â€“ equality check

ğŸª„ In short:
@dataclass makes it faster and cleaner to define simple classes that store data â€” no need to write repetitive code.


---------------------------------------DATACLASS AND PYDANTIC----------------------------
Purpose
dataclass is used to create simple data containers with less code.
Pydantic is used to validate and parse data (especially from external sources like APIs or JSON).

Type Checking
dataclass does not enforce types at runtime â€” type hints are just for readability.
Pydantic checks and enforces types at runtime, converting or rejecting invalid data.

Validation
dataclass has no built-in validation. You must write custom checks.
Pydantic automatically validates inputs and raises errors if data is incorrect.

Automatic Conversion
dataclass wonâ€™t convert "25" (string) to 25 (int).
Pydantic automatically converts compatible types.

Error Handling
dataclass wonâ€™t stop you from passing wrong types.
Pydantic raises detailed validation errors.

Performance
dataclass is faster and lighter (no validation overhead).
Pydantic is slightly slower because it performs runtime validation.

Use Case
Use dataclass for internal data storage or simple models.
Use Pydantic for external data, user input, or API responses that need validation.

ğŸ‘‰ In short:
dataclass = simple and lightweight.
Pydantic = powerful and safe with validation.

-------------data class built-in h or pydantic ki library import install krni parti h
---dataclass m bhi methods bna skty hn  or class method or class variable bhi define krskty hn dataclass m


----------An instance variable is a variable that belongs to a specific object (instance) of a class in Python.
ğŸ§© Explanation:
Each time you create an object from a class, it gets its own copy of the instance variables.
These variables are usually defined inside the __init__() method using self.

-------------------------------------ğŸ§° Example:---------------------------------
class Car:
    def __init__(self, brand, color):
        self.brand = brand      # instance variable
        self.color = color      # instance variable

# creating two objects
car1 = Car("Toyota", "Red")
car2 = Car("Honda", "Blue")

print(car1.brand)  # Toyota
print(car2.brand)  # Honda


Here:
brand and color are instance variables.
car1 and car2 each have their own separate values for these variables.

ğŸª„ In short:
An instance variable stores data unique to each object created from a class.


---------class method ko class ky sth call krskty hn
--------instance method keliye class ka phly ham instance bnaty hn or instance bnany ky bd phr ham usy use krhy hoty hn


------------------------------------------------------------------
ğŸ§  1. Instance Variable
Belongs to a specific object (instance) of a class.
Defined using self inside __init__().
Each object has its own copy of these variables.

âœ… Example:

class Car:
    def __init__(self, brand):
        self.brand = brand  # instance variable

ğŸ·ï¸ 2. Class Variable
Shared by all objects of the class.
Defined outside any method, usually directly under the class.
Changing it affects all instances (unless overridden).

âœ… Example:

class Car:
    wheels = 4  # class variable
    def __init__(self, brand):
        self.brand = brand  # instance variable

âš™ï¸ 3. Instance Method
Works with instance variables.
Needs access to the object (self).
Used to define behavior for individual objects.

âœ… Example:

class Car:
    def start(self):  # instance method
        print("Car started")

ğŸ§© 4. Class Method
Works with class variables, not instance variables.
Defined using the @classmethod decorator.
Takes cls (class reference) instead of self.

âœ… Example:

class Car:
    wheels = 4

    @classmethod
    def change_wheels(cls, new_count):
        cls.wheels = new_count

----------------------------------------
ğŸ”¹ Instance Variable
Belongs to one specific object.
Defined inside __init__() using self.
Each object has its own copy.

ğŸ”¹ Class Variable
Shared by all objects of the class.
Defined outside any method (inside the class).
Changing it affects all instances.

ğŸ”¹ Instance Method
Works with instance variables.
Takes self as the first parameter.
Defines behavior for individual objects.

ğŸ”¹ Class Method
Works with class variables.
Takes cls as the first parameter.
Defined using @classmethod.
Affects the entire class, not just one object.


ğŸ‘‰ Simple Summary:
Instance variable/method â†’ specific to each object.
Class variable/method â†’ shared by all objects.



-----------------------------------------------------------
