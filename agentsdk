
-----------agent jo h wo autonomous action lene wala ek model h--------------------------
------async mtlb ham request bhjty hn wo process krta h or jb uska process khtm hoga wo response dedega-------------------------
--------openai ky sdk ko use krty huy ham apny customize llm ko use krty hn jese openai m gemini flash use krty-----------------sdk agent or sary ky sary features sdk ky hongy lekin 
backend per jo llm hoga wo free wala hoga gemini ka-------------demagh jo h wo gemini ka or jo decision making wagaira h wo openai sdk ka h
-----------sync mtlb task jb perform horha h ussi wqt response do , or async mtlb 10 15 mnt bhi sochlo or jb response complete hojye tb response dedo
--------Runner m bht sary function method hoty by default async hota or ham Runner.run_sync krty hn (run_sync method h Runner m)(client to server communication jo krwaty hn wo sary 
tareeky runner m mojud hn)
------sync mtlb mene apko kaha biryani lekr ao to jb tk ap biryani ni laogy tb tk m wait krungi kisi  se bt ni krungi intezar krungi or async mtlb mene kaha ap biryani lekr ao or jb tk ap
ni aty tb tk m baqi kaam nipta lungi bt krlungi sbse kaam krlungi




-----------------------
In the OpenAI Agents SDK (and similar frameworks), you‚Äôll often see three levels of logic:
Global level ‚Üí Agent level ‚Üí Run level

Let‚Äôs break each down simply üëá
üåç Global Level
The topmost layer ‚Äî things that apply to the whole app or system, not just one agent or one conversation.
It defines shared settings, global tools, and event handlers that affect everything.

Think of it as:
üß† ‚ÄúWhat rules or tools should all agents or runs in my app have access to?‚Äù

Examples:
Registering a tool that every agent can use (like a database connector).
Defining logging or error-handling behavior for the whole app.
Setting up startup events, environment variables, or authentication.

üß© Agent Level
Defines the behavior and configuration of one specific agent.
Example: system prompt, tool access, goals, or memory setup.
Agents can inherit or override global settings.

Think of it as:
ü§ñ ‚ÄúWho the agent is and what it can do.‚Äù

‚öôÔ∏è Run Level
The execution layer ‚Äî what happens in a single conversation or task run.
This is where the agent actually processes input, calls tools, and produces outputs.

Think of it as:
üèÉ ‚ÄúWhat happens right now while the agent is running.‚Äù





---------------------SYNC FUNCTION IN RUNNER--------------(WORK ON GOOGLE COLAB)--------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

math_agent: Agent = Agent(name="MathAgent",
                     instructions="You are a helpful math assistant.",
                     model=llm_model) # gemini-2.5 as agent brain - chat completions

result: Runner = Runner.run_sync(math_agent, "why learn math for AI Agents?", run_config=config)

print("\nCALLING AGENT\n")
print(result.final_output)




-------------------------------------------------------------------------------------------------------------------------------------
-------------ASYNC FUNCTION IN RUNNER----------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio(is library se koi bhi async function chla skty hn)

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)---(run time configuration)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = await Runner.run(agent, "Tell me about recursion in programming.", run_config=config)-----------(.run ka method Runner m by-default async hi hota h)( or await mtlb jo kam 
krrha h uska wait krega or jb wo response ajyga usko result m dal dega)
    print(result.final_output)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.

if __name__ = "__main__":(is line ki zrort .py file m hoti h jo vs or cursor m hota h yhn iski zrort ni h kunke google colab m import asyncio krdia h hamne jo async ky sary function chlata ha
asyncio.run(main())



--------------chatgpt foran reply krta h to wo async h agr kisi kam per soch kr phr response kry to wo sync

-------------------STREAMED FUNCTION IN RUNNER-----------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
from agents import Agent, Runner,RunConfig, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from agents.run import RunConfig
import asyncio

from google.colab import userdata
gemini_api_key = userdata.get("GEMINI_API_KEY")

# Tracing disabled
set_tracing_disabled(disabled=True)

# 1. Which LLM Service?
external_client: AsyncOpenAI = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# 2. Which LLM Model?
llm_model: OpenAIChatCompletionsModel = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

config = RunConfig(
    model = llm_model,
    model_provider = external_client,
    tracing_disabled= True
)

async def main():
    agent = Agent(
        name = "Assistant",
        instructions="You only respond in haikus."
    )

    result = Runner.run_streamed(agent, "Tell me about recursion in programming.", run_config=config)
    print(result)
    async for e in result.stream_events():
      print(e)
    # Recursion
    # A function calls self,
    # Looping in smaller pieces,
    # Endless by design.


asyncio.run(main())


-----------------------------------streamed code run in google colab---------------------------
!pip install -Uq openai-agents
import nest_asyncio
nest_asyncio.apply()
import os
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
from agents.run import RunConfig
from google.colab import userdata
import asyncio

from agents import (
    Agent,
    Runner,
    set_default_openai_api,
    set_default_openai_client,
    set_tracing_disabled,
)

gemini_api_key = userdata.get("GEMINI_API_KEY")


# Check if the API key is present; if not, raise an error
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY is not set. Please ensure it is defined in your .env file.")

#Reference: https://ai.google.dev/gemini-api/docs/openai
external_client = AsyncOpenAI(
    api_key=gemini_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

model = OpenAIChatCompletionsModel(
    model="gemini-2.5-flash",
    openai_client=external_client
)

set_default_openai_client(client=external_client, use_for_tracing=False)
set_default_openai_api("chat_completions")
set_tracing_disabled(disabled=True)

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner


async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
        model=model
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):-----------------(ResponseTextDeltaEvent: event data ky class ki type bhi hogi yh type h 
data ki jo wo dega,raw_response_event:loop lgaya h jo us betahasha chizain arhi hn to hamne kaha raw response event jb hoga us wqt wo kam complete krke data return krta h)
            print(event.data.delta, end="", flush=True)-----------(event.data.delta: yh text nikalyga jitna complete hua h
 ( end=""----: by default print jb horha hota h to wo \n se next line per ajata h to hamne kehdia ky next line ni chaiye jo arha h usy dusry ky agay likhty jao space dal kr

asyncio.run(main())



---stream ky ilawa event bhi perform horhy hoty h kunke agent ko hamne task dia to wo bht sary kaam krrha hota ha
------In Python, flush=True in the print() function forces the output to be written to the console immediately. Normally Python buffers output, which means it might delay displaying 
what you printed until the buffer is full or a newline character is encountered. Using flush=True is particularly useful in scenarios like real-time logging or when printing streaming
data, ensuring that every piece of output is displayed as soon as it's generated.

For example, in your code:
print(event.data.delta.end="", flush=True)

end="": This means no newline character is appended at the end of the printed text.
flush=True: This forces the output to be flushed to the console immediately, even without a newline

------------------------FLUSH=TURE IN OUR CODE JO UPR GYA H------------------------
üß† Meaning of flush=True
Normally, Python buffers printed text ‚Äî meaning it stores it temporarily before actually showing it on the screen.
So when you print inside a loop, the text might not appear immediately.
‚úÖ flush=True forces Python to immediately push (flush) the text from the buffer to the console or output stream.

üí¨ In this context (streaming output)
Here, the model sends chunks of text as it generates them (event.data.delta = partial text).
To make the output appear live, like real-time typing, we use:
flush=True

That way, each small piece (token or word) appears instantly instead of waiting until the model finishes.

‚öôÔ∏è Example to visualize it
Without flush:
# Text appears all at once at the end
print("H", end="")
print("e", end="")
print("l", end="")
print("l", end="")
print("o", end="")

‚Üí Output might show ‚ÄúHello‚Äù only after the loop ends.
With flush:
# Text appears letter by letter
print("H", end="", flush=True)
print("e", end="", flush=True)
print("l", end="", flush=True)
print("l", end="", flush=True)
print("o", end="", flush=True)

‚Üí Output shows H e l l o in real-time.

üßæ Summary
flush=True ‚Üí forces the output to appear immediately.
Used in streaming to show model text live as it‚Äôs generated.
Without it, the console might delay or group text output.

‚úÖ In short:
flush=True makes the text appear instantly during streaming ‚Äî creating that smooth, ‚Äútyping live‚Äù effect.





--------------------------------------------------------------------------------------------
-------------------IMPORTANT:
That‚Äôs one of the 3 main ways to run an agent in the Agents SDK:

run_sync ‚Üí normal, blocking
run_async ‚Üí async, non-blocking
run_streamed ‚Üí streaming, real-time output


‚ö° What is ‚Äústreaming‚Äù?
Streaming means:
The model starts sending partial output while it‚Äôs still generating the rest.
So instead of waiting for the whole response (like sync/async do), your code receives chunks (events) of the output as soon as they‚Äôre available.

Think of it like:
Non-streamed (sync/async): Wait till the whole essay is done ‚Üí print once.
Streamed: Show words as they‚Äôre typed ‚Üí print gradually, real-time.

üß† Runner.run_streamed() ‚Äî what it does
It starts an asynchronous run.
Immediately returns a streaming result object (not the final text).
That result object emits events as the model generates output.

---------------You can then do:CODE-----------------

async for e in result.stream_events():
    print(e)

‚Üí This listens for events like tokens, messages, or tool calls as they happen.
This is perfect for live chat UIs (like Chainlit, Gradio, or Colab outputs).

------------------------------------------------------------------------------

üîÑ Runner.run_async() ‚Äî what it does
Runs the same logic but waits until the full response is ready before returning.
You can await it to get the entire message result directly.
No live token-by-token updates.

Example:

result = await Runner.run_async(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you just need the full answer, not a live stream.
--------------------------------------------------------------------------------------

‚è≥ Runner.run_sync() ‚Äî what it does
Same as run_async but for non-async (normal) Python code.
Blocks until the full run completes (like a normal function).

Example:

result = Runner.run_sync(agent, "Hello!", run_config=config)
print(result.final_output)

Use it when: you‚Äôre in a plain Python script (no asyncio).




---------------------------------------------------------------------
üí¨ In your code:

result = Runner.run_streamed(agent, "Tell me about recursion...", run_config=config)

Starts the agent.
Doesn‚Äôt wait for full completion.
Gives you a stream handle in result.

Then this part:

async for e in result.stream_events():
    print(e)

prints the tokens or partial responses live, as they arrive.
That‚Äôs how you get ‚Äúlive typing‚Äù or real-time generation.


---------------------------------------------
üß† What is Runner?
Runner is a controller class in the OpenAI Agents SDK that‚Äôs responsible for executing (running) an agent.
Think of it as the engine that takes:
your Agent (the brain),
a user input (the question),
a RunConfig (the setup),
and actually runs the full conversation or reasoning process.

‚öôÔ∏è In short:
Runner = the component that manages how an agent‚Äôs logic is executed ‚Äî including tool calls, message handling, and LLM requests.
---runner jo h wo client to server communication krwata ha or communication sync hogi asyn hogi ya streamed hogi yh define krty.


---------------------------------------
üß± Analogy ‚Äî AI system as a car
Agent ‚Üí The driver ‚Äî knows what to do and how to respond.
RunConfig ‚Üí The GPS/settings ‚Äî defines which model, API key, and configuration to use.
Runner ‚Üí The engine ‚Äî actually runs the process and makes the agent work.
Without the Runner, the agent just exists but can‚Äôt perform any actions.

üß∞ Runner‚Äôs Main Methods
run_sync() ‚Üí Runs the agent normally (blocking execution until done).
run_async() ‚Üí Runs the agent asynchronously using await (non-blocking).
run_streamed() ‚Üí Runs the agent in streaming mode, giving partial outputs live while generating.
